<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Principal Component Analysis | Principal Component Analysis: Unveiling the Structure of Psychological Data</title>
  <meta name="description" content="A comprehensive handbook on psychological measurement concepts, theories, and statistical methods." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Principal Component Analysis | Principal Component Analysis: Unveiling the Structure of Psychological Data" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A comprehensive handbook on psychological measurement concepts, theories, and statistical methods." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Principal Component Analysis | Principal Component Analysis: Unveiling the Structure of Psychological Data" />
  
  <meta name="twitter:description" content="A comprehensive handbook on psychological measurement concepts, theories, and statistical methods." />
  

<meta name="author" content="Psychological Measurement" />


<meta name="date" content="2025-06-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="validity-and-validation-tests.html"/>
<link rel="next" href="chi-square.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Handbook of Psychological Measurement</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-use-this-book"><i class="fa fa-check"></i>How to Use This Book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#required-r-packages"><i class="fa fa-check"></i>Required R Packages</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="classical-test-theory.html"><a href="classical-test-theory.html"><i class="fa fa-check"></i><b>1</b> Classical Test Theory: The Foundation of Psychological Measurement</a>
<ul>
<li class="chapter" data-level="1.1" data-path="classical-test-theory.html"><a href="classical-test-theory.html#understanding-classical-test-theory-the-building-blocks-of-measurement"><i class="fa fa-check"></i><b>1.1</b> Understanding Classical Test Theory: The Building Blocks of Measurement</a></li>
<li class="chapter" data-level="1.2" data-path="classical-test-theory.html"><a href="classical-test-theory.html#the-fundamental-equation-breaking-down-test-scores"><i class="fa fa-check"></i><b>1.2</b> The Fundamental Equation: Breaking Down Test Scores</a></li>
<li class="chapter" data-level="1.3" data-path="classical-test-theory.html"><a href="classical-test-theory.html#core-assumptions-the-rules-of-the-game"><i class="fa fa-check"></i><b>1.3</b> Core Assumptions: The Rules of the Game</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="classical-test-theory.html"><a href="classical-test-theory.html#assumption-1-true-score-as-expected-value"><i class="fa fa-check"></i><b>1.3.1</b> Assumption 1: True Score as Expected Value</a></li>
<li class="chapter" data-level="1.3.2" data-path="classical-test-theory.html"><a href="classical-test-theory.html#assumption-2-error-properties"><i class="fa fa-check"></i><b>1.3.2</b> Assumption 2: Error Properties</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="classical-test-theory.html"><a href="classical-test-theory.html#visualizing-classical-test-theory-concepts"><i class="fa fa-check"></i><b>1.4</b> Visualizing Classical Test Theory Concepts</a></li>
<li class="chapter" data-level="1.5" data-path="classical-test-theory.html"><a href="classical-test-theory.html#reliability-the-consistency-of-measurement"><i class="fa fa-check"></i><b>1.5</b> Reliability: The Consistency of Measurement</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="classical-test-theory.html"><a href="classical-test-theory.html#the-reliability-coefficient"><i class="fa fa-check"></i><b>1.5.1</b> The Reliability Coefficient</a></li>
<li class="chapter" data-level="1.5.2" data-path="classical-test-theory.html"><a href="classical-test-theory.html#alternative-reliability-formula"><i class="fa fa-check"></i><b>1.5.2</b> Alternative Reliability Formula</a></li>
<li class="chapter" data-level="1.5.3" data-path="classical-test-theory.html"><a href="classical-test-theory.html#types-of-reliability-evidence"><i class="fa fa-check"></i><b>1.5.3</b> Types of Reliability Evidence</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="classical-test-theory.html"><a href="classical-test-theory.html#demonstrating-reliability-concepts"><i class="fa fa-check"></i><b>1.6</b> Demonstrating Reliability Concepts</a></li>
<li class="chapter" data-level="1.7" data-path="classical-test-theory.html"><a href="classical-test-theory.html#validity-measuring-what-we-intend-to-measure"><i class="fa fa-check"></i><b>1.7</b> Validity: Measuring What We Intend to Measure</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="classical-test-theory.html"><a href="classical-test-theory.html#the-reliability-validity-relationship"><i class="fa fa-check"></i><b>1.7.1</b> The Reliability-Validity Relationship</a></li>
<li class="chapter" data-level="1.7.2" data-path="classical-test-theory.html"><a href="classical-test-theory.html#correction-for-attenuation"><i class="fa fa-check"></i><b>1.7.2</b> Correction for Attenuation</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="classical-test-theory.html"><a href="classical-test-theory.html#standard-error-of-measurement"><i class="fa fa-check"></i><b>1.8</b> Standard Error of Measurement</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="classical-test-theory.html"><a href="classical-test-theory.html#confidence-intervals-for-individual-scores"><i class="fa fa-check"></i><b>1.8.1</b> Confidence Intervals for Individual Scores</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="classical-test-theory.html"><a href="classical-test-theory.html#applications-across-psychological-domains"><i class="fa fa-check"></i><b>1.9</b> Applications Across Psychological Domains</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="classical-test-theory.html"><a href="classical-test-theory.html#clinical-psychology-applications"><i class="fa fa-check"></i><b>1.9.1</b> Clinical Psychology Applications</a></li>
<li class="chapter" data-level="1.9.2" data-path="classical-test-theory.html"><a href="classical-test-theory.html#cognitive-psychology-applications"><i class="fa fa-check"></i><b>1.9.2</b> Cognitive Psychology Applications</a></li>
<li class="chapter" data-level="1.9.3" data-path="classical-test-theory.html"><a href="classical-test-theory.html#behavioral-psychology-applications"><i class="fa fa-check"></i><b>1.9.3</b> Behavioral Psychology Applications</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="classical-test-theory.html"><a href="classical-test-theory.html#limitations-and-modern-developments"><i class="fa fa-check"></i><b>1.10</b> Limitations and Modern Developments</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="classical-test-theory.html"><a href="classical-test-theory.html#classical-test-theory-limitations"><i class="fa fa-check"></i><b>1.10.1</b> Classical Test Theory Limitations</a></li>
<li class="chapter" data-level="1.10.2" data-path="classical-test-theory.html"><a href="classical-test-theory.html#modern-alternatives"><i class="fa fa-check"></i><b>1.10.2</b> Modern Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="classical-test-theory.html"><a href="classical-test-theory.html#practical-guidelines-for-researchers-and-practitioners"><i class="fa fa-check"></i><b>1.11</b> Practical Guidelines for Researchers and Practitioners</a>
<ul>
<li class="chapter" data-level="1.11.1" data-path="classical-test-theory.html"><a href="classical-test-theory.html#test-selection-criteria"><i class="fa fa-check"></i><b>1.11.1</b> Test Selection Criteria</a></li>
<li class="chapter" data-level="1.11.2" data-path="classical-test-theory.html"><a href="classical-test-theory.html#score-interpretation-guidelines"><i class="fa fa-check"></i><b>1.11.2</b> Score Interpretation Guidelines</a></li>
<li class="chapter" data-level="1.11.3" data-path="classical-test-theory.html"><a href="classical-test-theory.html#test-development-recommendations"><i class="fa fa-check"></i><b>1.11.3</b> Test Development Recommendations</a></li>
</ul></li>
<li class="chapter" data-level="1.12" data-path="classical-test-theory.html"><a href="classical-test-theory.html#references"><i class="fa fa-check"></i><b>1.12</b> References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="reliability.html"><a href="reliability.html"><i class="fa fa-check"></i><b>2</b> Reliability in Classical Test Theory</a>
<ul>
<li class="chapter" data-level="2.1" data-path="reliability.html"><a href="reliability.html#parallel-tests-and-their-mathematical-properties"><i class="fa fa-check"></i><b>2.1</b> Parallel Tests and Their Mathematical Properties</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="reliability.html"><a href="reliability.html#definition-of-parallel-tests"><i class="fa fa-check"></i><b>2.1.1</b> Definition of Parallel Tests</a></li>
<li class="chapter" data-level="2.1.2" data-path="reliability.html"><a href="reliability.html#mathematical-implications"><i class="fa fa-check"></i><b>2.1.2</b> Mathematical Implications</a></li>
<li class="chapter" data-level="2.1.3" data-path="reliability.html"><a href="reliability.html#tau-equivalent-tests"><i class="fa fa-check"></i><b>2.1.3</b> Tau-Equivalent Tests</a></li>
<li class="chapter" data-level="2.1.4" data-path="reliability.html"><a href="reliability.html#essentially-tau-equivalent-tests"><i class="fa fa-check"></i><b>2.1.4</b> Essentially Tau-Equivalent Tests</a></li>
<li class="chapter" data-level="2.1.5" data-path="reliability.html"><a href="reliability.html#example-analyzing-parallel-tests"><i class="fa fa-check"></i><b>2.1.5</b> Example: Analyzing Parallel Tests</a></li>
<li class="chapter" data-level="2.1.6" data-path="reliability.html"><a href="reliability.html#r-code-for-parallel-test-analysis"><i class="fa fa-check"></i><b>2.1.6</b> R Code for Parallel Test Analysis</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="reliability.html"><a href="reliability.html#reliability-and-test-length"><i class="fa fa-check"></i><b>2.2</b> Reliability and Test Length</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="reliability.html"><a href="reliability.html#the-spearman-brown-prophecy-formula"><i class="fa fa-check"></i><b>2.2.1</b> The Spearman-Brown Prophecy Formula</a></li>
<li class="chapter" data-level="2.2.2" data-path="reliability.html"><a href="reliability.html#special-cases"><i class="fa fa-check"></i><b>2.2.2</b> Special Cases</a></li>
<li class="chapter" data-level="2.2.3" data-path="reliability.html"><a href="reliability.html#example-effect-of-test-length-on-reliability"><i class="fa fa-check"></i><b>2.2.3</b> Example: Effect of Test Length on Reliability</a></li>
<li class="chapter" data-level="2.2.4" data-path="reliability.html"><a href="reliability.html#r-code-for-test-length-analysis"><i class="fa fa-check"></i><b>2.2.4</b> R Code for Test Length Analysis</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="reliability.html"><a href="reliability.html#reliability-and-group-homogeneity"><i class="fa fa-check"></i><b>2.3</b> Reliability and Group Homogeneity</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="reliability.html"><a href="reliability.html#the-effect-of-range-restriction"><i class="fa fa-check"></i><b>2.3.1</b> The Effect of Range Restriction</a></li>
<li class="chapter" data-level="2.3.2" data-path="reliability.html"><a href="reliability.html#adjusting-reliability-for-range-restriction"><i class="fa fa-check"></i><b>2.3.2</b> Adjusting Reliability for Range Restriction</a></li>
<li class="chapter" data-level="2.3.3" data-path="reliability.html"><a href="reliability.html#example-effect-of-range-restriction"><i class="fa fa-check"></i><b>2.3.3</b> Example: Effect of Range Restriction</a></li>
<li class="chapter" data-level="2.3.4" data-path="reliability.html"><a href="reliability.html#r-code-for-range-restriction-analysis"><i class="fa fa-check"></i><b>2.3.4</b> R Code for Range Restriction Analysis</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="reliability.html"><a href="reliability.html#estimation-of-true-scores"><i class="fa fa-check"></i><b>2.4</b> Estimation of True Scores</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="reliability.html"><a href="reliability.html#regression-toward-the-mean"><i class="fa fa-check"></i><b>2.4.1</b> Regression Toward the Mean</a></li>
<li class="chapter" data-level="2.4.2" data-path="reliability.html"><a href="reliability.html#confidence-intervals-for-estimated-true-scores"><i class="fa fa-check"></i><b>2.4.2</b> Confidence Intervals for Estimated True Scores</a></li>
<li class="chapter" data-level="2.4.3" data-path="reliability.html"><a href="reliability.html#example-estimating-true-scores"><i class="fa fa-check"></i><b>2.4.3</b> Example: Estimating True Scores</a></li>
<li class="chapter" data-level="2.4.4" data-path="reliability.html"><a href="reliability.html#r-code-for-true-score-estimation"><i class="fa fa-check"></i><b>2.4.4</b> R Code for True Score Estimation</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="reliability.html"><a href="reliability.html#correction-for-attenuation-1"><i class="fa fa-check"></i><b>2.5</b> Correction for Attenuation</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="reliability.html"><a href="reliability.html#the-attenuation-problem"><i class="fa fa-check"></i><b>2.5.1</b> The Attenuation Problem</a></li>
<li class="chapter" data-level="2.5.2" data-path="reliability.html"><a href="reliability.html#the-correction-formula"><i class="fa fa-check"></i><b>2.5.2</b> The Correction Formula</a></li>
<li class="chapter" data-level="2.5.3" data-path="reliability.html"><a href="reliability.html#example-correcting-a-correlation"><i class="fa fa-check"></i><b>2.5.3</b> Example: Correcting a Correlation</a></li>
<li class="chapter" data-level="2.5.4" data-path="reliability.html"><a href="reliability.html#r-code-for-attenuation-correction"><i class="fa fa-check"></i><b>2.5.4</b> R Code for Attenuation Correction</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="reliability.html"><a href="reliability.html#methods-for-estimating-reliability"><i class="fa fa-check"></i><b>2.6</b> Methods for Estimating Reliability</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="reliability.html"><a href="reliability.html#test-retest-reliability"><i class="fa fa-check"></i><b>2.6.1</b> Test-Retest Reliability</a></li>
<li class="chapter" data-level="2.6.2" data-path="reliability.html"><a href="reliability.html#internal-consistency-reliability"><i class="fa fa-check"></i><b>2.6.2</b> Internal Consistency Reliability</a></li>
<li class="chapter" data-level="2.6.3" data-path="reliability.html"><a href="reliability.html#r-code-for-reliability-estimation"><i class="fa fa-check"></i><b>2.6.3</b> R Code for Reliability Estimation</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="reliability.html"><a href="reliability.html#practical-applications-and-recommendations"><i class="fa fa-check"></i><b>2.7</b> Practical Applications and Recommendations</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="reliability.html"><a href="reliability.html#minimum-reliability-standards"><i class="fa fa-check"></i><b>2.7.1</b> Minimum Reliability Standards</a></li>
<li class="chapter" data-level="2.7.2" data-path="reliability.html"><a href="reliability.html#strategies-for-improving-reliability"><i class="fa fa-check"></i><b>2.7.2</b> Strategies for Improving Reliability</a></li>
<li class="chapter" data-level="2.7.3" data-path="reliability.html"><a href="reliability.html#reporting-practices"><i class="fa fa-check"></i><b>2.7.3</b> Reporting Practices</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="reliability.html"><a href="reliability.html#references-1"><i class="fa fa-check"></i><b>2.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="generalizability-theory-in-psychological-measurement.html"><a href="generalizability-theory-in-psychological-measurement.html"><i class="fa fa-check"></i><b>3</b> Generalizability Theory in Psychological Measurement</a>
<ul>
<li class="chapter" data-level="3.1" data-path="generalizability-theory-in-psychological-measurement.html"><a href="generalizability-theory-in-psychological-measurement.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="generalizability-theory-in-psychological-measurement.html"><a href="generalizability-theory-in-psychological-measurement.html#basic-concepts-of-generalizability-theory"><i class="fa fa-check"></i><b>3.2</b> Basic Concepts of Generalizability Theory</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="generalizability-theory-in-psychological-measurement.html"><a href="generalizability-theory-in-psychological-measurement.html#universe-of-admissible-observations"><i class="fa fa-check"></i><b>3.2.1</b> Universe of Admissible Observations</a></li>
<li class="chapter" data-level="3.2.2" data-path="generalizability-theory-in-psychological-measurement.html"><a href="generalizability-theory-in-psychological-measurement.html#facets"><i class="fa fa-check"></i><b>3.2.2</b> Facets</a></li>
<li class="chapter" data-level="3.2.3" data-path="generalizability-theory-in-psychological-measurement.html"><a href="generalizability-theory-in-psychological-measurement.html#variance-components"><i class="fa fa-check"></i><b>3.2.3</b> Variance Components</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="generalizability-theory-in-psychological-measurement.html"><a href="generalizability-theory-in-psychological-measurement.html#one-facet-designs"><i class="fa fa-check"></i><b>3.3</b> One-Facet Designs</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="generalizability-theory-in-psychological-measurement.html"><a href="generalizability-theory-in-psychological-measurement.html#crossed-design-p-i"><i class="fa fa-check"></i><b>3.3.1</b> Crossed Design (p × i)</a></li>
<li class="chapter" data-level="3.3.2" data-path="generalizability-theory-in-psychological-measurement.html"><a href="generalizability-theory-in-psychological-measurement.html#nested-design-ip"><i class="fa fa-check"></i><b>3.3.2</b> Nested Design (i:p)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="generalizability-theory-in-psychological-measurement.html"><a href="generalizability-theory-in-psychological-measurement.html#two-facet-designs"><i class="fa fa-check"></i><b>3.4</b> Two-Facet Designs</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="generalizability-theory-in-psychological-measurement.html"><a href="generalizability-theory-in-psychological-measurement.html#crossed-design-p-i-j"><i class="fa fa-check"></i><b>3.4.1</b> Crossed Design (p × i × j)</a></li>
<li class="chapter" data-level="3.4.2" data-path="generalizability-theory-in-psychological-measurement.html"><a href="generalizability-theory-in-psychological-measurement.html#nested-two-facet-design-p-ij"><i class="fa fa-check"></i><b>3.4.2</b> Nested Two-Facet Design (p × (i:j))</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="generalizability-theory-in-psychological-measurement.html"><a href="generalizability-theory-in-psychological-measurement.html#decision-studies-d-studies"><i class="fa fa-check"></i><b>3.5</b> Decision Studies (D-Studies)</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="generalizability-theory-in-psychological-measurement.html"><a href="generalizability-theory-in-psychological-measurement.html#generalizability-coefficients"><i class="fa fa-check"></i><b>3.5.1</b> Generalizability Coefficients</a></li>
<li class="chapter" data-level="3.5.2" data-path="generalizability-theory-in-psychological-measurement.html"><a href="generalizability-theory-in-psychological-measurement.html#example-optimizing-assessment-design"><i class="fa fa-check"></i><b>3.5.2</b> Example: Optimizing Assessment Design</a></li>
<li class="chapter" data-level="3.5.3" data-path="generalizability-theory-in-psychological-measurement.html"><a href="generalizability-theory-in-psychological-measurement.html#relative-versus-absolute-decisions"><i class="fa fa-check"></i><b>3.5.3</b> Relative versus Absolute Decisions</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="generalizability-theory-in-psychological-measurement.html"><a href="generalizability-theory-in-psychological-measurement.html#practical-applications-in-psychology"><i class="fa fa-check"></i><b>3.6</b> Practical Applications in Psychology</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="generalizability-theory-in-psychological-measurement.html"><a href="generalizability-theory-in-psychological-measurement.html#clinical-assessment"><i class="fa fa-check"></i><b>3.6.1</b> Clinical Assessment</a></li>
<li class="chapter" data-level="3.6.2" data-path="generalizability-theory-in-psychological-measurement.html"><a href="generalizability-theory-in-psychological-measurement.html#educational-measurement"><i class="fa fa-check"></i><b>3.6.2</b> Educational Measurement</a></li>
<li class="chapter" data-level="3.6.3" data-path="generalizability-theory-in-psychological-measurement.html"><a href="generalizability-theory-in-psychological-measurement.html#research-methodology"><i class="fa fa-check"></i><b>3.6.3</b> Research Methodology</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="generalizability-theory-in-psychological-measurement.html"><a href="generalizability-theory-in-psychological-measurement.html#conclusion"><i class="fa fa-check"></i><b>3.7</b> Conclusion</a></li>
<li class="chapter" data-level="3.8" data-path="generalizability-theory-in-psychological-measurement.html"><a href="generalizability-theory-in-psychological-measurement.html#references-2"><i class="fa fa-check"></i><b>3.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html"><i class="fa fa-check"></i><b>4</b> Models for Dichotomous Items</a>
<ul>
<li class="chapter" data-level="4.1" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#understanding-dichotomous-items"><i class="fa fa-check"></i><b>4.1.1</b> 1. Understanding Dichotomous Items</a></li>
<li class="chapter" data-level="4.1.2" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#example-from-cognitive-psychology"><i class="fa fa-check"></i><b>4.1.2</b> Example from Cognitive Psychology</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#the-binomial-model"><i class="fa fa-check"></i><b>4.2</b> 2. The Binomial Model</a></li>
<li class="chapter" data-level="4.3" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#mathematical-definition"><i class="fa fa-check"></i><b>4.3</b> Mathematical Definition</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#interpretation"><i class="fa fa-check"></i><b>4.3.1</b> Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#variance-and-error"><i class="fa fa-check"></i><b>4.4</b> Variance and Error</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#worked-example"><i class="fa fa-check"></i><b>4.4.1</b> Worked Example</a></li>
<li class="chapter" data-level="4.4.2" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#plotting-the-binomial-distribution"><i class="fa fa-check"></i><b>4.4.2</b> Plotting the Binomial Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#generalized-binomial-model"><i class="fa fa-check"></i><b>4.5</b> 3. Generalized Binomial Model</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#mathematical-formulation"><i class="fa fa-check"></i><b>4.5.1</b> Mathematical Formulation</a></li>
<li class="chapter" data-level="4.5.2" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#understanding-the-components"><i class="fa fa-check"></i><b>4.5.2</b> Understanding the Components</a></li>
<li class="chapter" data-level="4.5.3" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#example-from-behavioral-psychology"><i class="fa fa-check"></i><b>4.5.3</b> Example from Behavioral Psychology</a></li>
<li class="chapter" data-level="4.5.4" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#advanced-item-response-functions"><i class="fa fa-check"></i><b>4.5.4</b> Advanced Item Response Functions</a></li>
<li class="chapter" data-level="4.5.5" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#information-functions-and-precision"><i class="fa fa-check"></i><b>4.5.5</b> Information Functions and Precision</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#psychological-applications-and-item-analysis"><i class="fa fa-check"></i><b>4.6</b> 4. Psychological Applications and Item Analysis</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#clinical-example"><i class="fa fa-check"></i><b>4.6.1</b> Clinical Example</a></li>
<li class="chapter" data-level="4.6.2" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#item-statistics"><i class="fa fa-check"></i><b>4.6.2</b> Item Statistics</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#reliability-and-estimation"><i class="fa fa-check"></i><b>4.7</b> 5. Reliability and Estimation</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#example-calculation"><i class="fa fa-check"></i><b>4.7.1</b> Example Calculation</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#the-rasch-model"><i class="fa fa-check"></i><b>4.8</b> 6. The Rasch Model</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#mathematical-foundation"><i class="fa fa-check"></i><b>4.8.1</b> Mathematical Foundation</a></li>
<li class="chapter" data-level="4.8.2" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#alternative-parameterization"><i class="fa fa-check"></i><b>4.8.2</b> Alternative Parameterization</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#key-properties-of-the-rasch-model"><i class="fa fa-check"></i><b>4.9</b> Key Properties of the Rasch Model</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#sufficiency-of-raw-scores"><i class="fa fa-check"></i><b>4.9.1</b> 1. Sufficiency of Raw Scores</a></li>
<li class="chapter" data-level="4.9.2" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#sample-independence"><i class="fa fa-check"></i><b>4.9.2</b> 2. Sample Independence</a></li>
<li class="chapter" data-level="4.9.3" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#specific-objectivity"><i class="fa fa-check"></i><b>4.9.3</b> 3. Specific Objectivity</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#practical-example-depression-assessment"><i class="fa fa-check"></i><b>4.10</b> Practical Example: Depression Assessment</a></li>
<li class="chapter" data-level="4.11" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#item-fit-analysis"><i class="fa fa-check"></i><b>4.11</b> Item Fit Analysis</a>
<ul>
<li class="chapter" data-level="4.11.1" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#infit-and-outfit-statistics"><i class="fa fa-check"></i><b>4.11.1</b> Infit and Outfit Statistics</a></li>
</ul></li>
<li class="chapter" data-level="4.12" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#two-parameter-and-three-parameter-logistic-models"><i class="fa fa-check"></i><b>4.12</b> 7. Two-Parameter and Three-Parameter Logistic Models</a>
<ul>
<li class="chapter" data-level="4.12.1" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#two-parameter-logistic-2pl-model"><i class="fa fa-check"></i><b>4.12.1</b> Two-Parameter Logistic (2PL) Model</a></li>
<li class="chapter" data-level="4.12.2" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#interpretation-of-parameters"><i class="fa fa-check"></i><b>4.12.2</b> Interpretation of Parameters</a></li>
<li class="chapter" data-level="4.12.3" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#three-parameter-logistic-3pl-model"><i class="fa fa-check"></i><b>4.12.3</b> Three-Parameter Logistic (3PL) Model</a></li>
<li class="chapter" data-level="4.12.4" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#comparative-example-intelligence-test-items"><i class="fa fa-check"></i><b>4.12.4</b> Comparative Example: Intelligence Test Items</a></li>
</ul></li>
<li class="chapter" data-level="4.13" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#testlet-models-and-local-dependence"><i class="fa fa-check"></i><b>4.13</b> 8. Testlet Models and Local Dependence</a>
<ul>
<li class="chapter" data-level="4.13.1" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#the-testlet-response-model"><i class="fa fa-check"></i><b>4.13.1</b> The Testlet Response Model</a></li>
<li class="chapter" data-level="4.13.2" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#example-reading-comprehension-assessment"><i class="fa fa-check"></i><b>4.13.2</b> Example: Reading Comprehension Assessment</a></li>
</ul></li>
<li class="chapter" data-level="4.14" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#differential-item-functioning-dif"><i class="fa fa-check"></i><b>4.14</b> 9. Differential Item Functioning (DIF)</a>
<ul>
<li class="chapter" data-level="4.14.1" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#types-of-dif"><i class="fa fa-check"></i><b>4.14.1</b> Types of DIF</a></li>
<li class="chapter" data-level="4.14.2" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#non-uniform-dif"><i class="fa fa-check"></i><b>4.14.2</b> Non-uniform DIF</a></li>
<li class="chapter" data-level="4.14.3" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#dif-detection-example"><i class="fa fa-check"></i><b>4.14.3</b> DIF Detection Example</a></li>
</ul></li>
<li class="chapter" data-level="4.15" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#computerized-adaptive-testing-cat"><i class="fa fa-check"></i><b>4.15</b> 10. Computerized Adaptive Testing (CAT)</a>
<ul>
<li class="chapter" data-level="4.15.1" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#cat-algorithm-components"><i class="fa fa-check"></i><b>4.15.1</b> CAT Algorithm Components</a></li>
<li class="chapter" data-level="4.15.2" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#information-based-item-selection"><i class="fa fa-check"></i><b>4.15.2</b> Information-Based Item Selection</a></li>
</ul></li>
<li class="chapter" data-level="4.16" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#cat-simulation-example"><i class="fa fa-check"></i><b>4.16</b> CAT Simulation Example</a></li>
<li class="chapter" data-level="4.17" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#model-comparison-and-selection"><i class="fa fa-check"></i><b>4.17</b> 11. Model Comparison and Selection</a>
<ul>
<li class="chapter" data-level="4.17.1" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#information-criteria"><i class="fa fa-check"></i><b>4.17.1</b> Information Criteria</a></li>
</ul></li>
<li class="chapter" data-level="4.18" data-path="models-for-dichotomous-items.html"><a href="models-for-dichotomous-items.html#practical-model-comparison"><i class="fa fa-check"></i><b>4.18</b> Practical Model Comparison</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="advanced-topics-and-future-directions.html"><a href="advanced-topics-and-future-directions.html"><i class="fa fa-check"></i><b>5</b> 12. Advanced Topics and Future Directions</a>
<ul>
<li class="chapter" data-level="5.1" data-path="advanced-topics-and-future-directions.html"><a href="advanced-topics-and-future-directions.html#multidimensional-irt"><i class="fa fa-check"></i><b>5.1</b> Multidimensional IRT</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="advanced-topics-and-future-directions.html"><a href="advanced-topics-and-future-directions.html#explanatory-irt"><i class="fa fa-check"></i><b>5.1.1</b> Explanatory IRT</a></li>
<li class="chapter" data-level="5.1.2" data-path="advanced-topics-and-future-directions.html"><a href="advanced-topics-and-future-directions.html#machine-learning-applications"><i class="fa fa-check"></i><b>5.1.2</b> Machine Learning Applications</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="advanced-topics-and-future-directions.html"><a href="advanced-topics-and-future-directions.html#conclusion-1"><i class="fa fa-check"></i><b>5.2</b> Conclusion</a></li>
<li class="chapter" data-level="5.3" data-path="advanced-topics-and-future-directions.html"><a href="advanced-topics-and-future-directions.html#references-3"><i class="fa fa-check"></i><b>5.3</b> References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html"><i class="fa fa-check"></i><b>6</b> Validity and Validation Tests</a>
<ul>
<li class="chapter" data-level="6.1" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#introduction-2"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#mathematical-foundations-of-validity-theory"><i class="fa fa-check"></i><b>6.2</b> Mathematical Foundations of Validity Theory</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#theoretical-framework-and-mathematical-models"><i class="fa fa-check"></i><b>6.2.1</b> Theoretical Framework and Mathematical Models</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#problems-of-validity"><i class="fa fa-check"></i><b>6.3</b> Problems of Validity</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#the-challenge-of-construct-representation"><i class="fa fa-check"></i><b>6.3.1</b> The Challenge of Construct Representation</a></li>
<li class="chapter" data-level="6.3.2" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#construct-underrepresentation-mathematical-analysis"><i class="fa fa-check"></i><b>6.3.2</b> Construct Underrepresentation: Mathematical Analysis</a></li>
<li class="chapter" data-level="6.3.3" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#construct-irrelevant-variance-mathematical-treatment"><i class="fa fa-check"></i><b>6.3.3</b> Construct-Irrelevant Variance: Mathematical Treatment</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#many-faces-of-validity-and-standards"><i class="fa fa-check"></i><b>6.4</b> Many Faces of Validity and Standards</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#historical-evolution-of-validity-frameworks"><i class="fa fa-check"></i><b>6.4.1</b> Historical Evolution of Validity Frameworks</a></li>
<li class="chapter" data-level="6.4.2" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#the-five-sources-of-validity-evidence"><i class="fa fa-check"></i><b>6.4.2</b> The Five Sources of Validity Evidence</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#sources-of-evidence-in-validity"><i class="fa fa-check"></i><b>6.5</b> Sources of Evidence in Validity</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#evidence-based-on-test-content-mathematical-framework"><i class="fa fa-check"></i><b>6.5.1</b> Evidence Based on Test Content: Mathematical Framework</a></li>
<li class="chapter" data-level="6.5.2" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#evidence-based-on-response-processes"><i class="fa fa-check"></i><b>6.5.2</b> Evidence Based on Response Processes</a></li>
<li class="chapter" data-level="6.5.3" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#evidence-based-on-internal-structure"><i class="fa fa-check"></i><b>6.5.3</b> Evidence Based on Internal Structure</a></li>
<li class="chapter" data-level="6.5.4" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#the-multitrait-multimethod-matrix"><i class="fa fa-check"></i><b>6.5.4</b> The Multitrait-Multimethod Matrix</a></li>
<li class="chapter" data-level="6.5.5" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#analyzing-mtmm-evidence"><i class="fa fa-check"></i><b>6.5.5</b> Analyzing MTMM Evidence</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#mathematical-theory-of-range-restriction-effects"><i class="fa fa-check"></i><b>6.6</b> Mathematical Theory of Range Restriction Effects</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#theoretical-framework-and-mathematical-foundations"><i class="fa fa-check"></i><b>6.6.1</b> Theoretical Framework and Mathematical Foundations</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#mathematical-theory-of-multitrait-multimethod-analysis"><i class="fa fa-check"></i><b>6.7</b> Mathematical Theory of Multitrait-Multimethod Analysis</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#theoretical-framework-for-mtmm-matrix-decomposition"><i class="fa fa-check"></i><b>6.7.1</b> Theoretical Framework for MTMM Matrix Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#mathematical-theory-of-bayesian-classification"><i class="fa fa-check"></i><b>6.8</b> Mathematical Theory of Bayesian Classification</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#theoretical-framework-for-optimal-decision-making"><i class="fa fa-check"></i><b>6.8.1</b> Theoretical Framework for Optimal Decision Making</a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#mathematical-theory-of-irt-based-validity"><i class="fa fa-check"></i><b>6.9</b> Mathematical Theory of IRT-Based Validity</a>
<ul>
<li class="chapter" data-level="6.9.1" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#theoretical-framework-for-item-response-theory-validation"><i class="fa fa-check"></i><b>6.9.1</b> Theoretical Framework for Item Response Theory Validation</a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#generate-angoff-ratings-probability-of-correct-response-for-borderline-candidate"><i class="fa fa-check"></i><b>6.10</b> Generate Angoff ratings (probability of correct response for borderline candidate)</a>
<ul>
<li class="chapter" data-level="6.10.1" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#section"><i class="fa fa-check"></i><b>6.10.1</b> </a></li>
<li class="chapter" data-level="6.10.2" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#judge-ratings-with-some-individual-differences"><i class="fa fa-check"></i><b>6.10.2</b> Judge ratings with some individual differences</a></li>
<li class="chapter" data-level="6.10.3" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#calculate-statistics"><i class="fa fa-check"></i><b>6.10.3</b> Calculate statistics</a></li>
<li class="chapter" data-level="6.10.4" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#overall-cut-score"><i class="fa fa-check"></i><b>6.10.4</b> Overall cut score</a></li>
</ul></li>
<li class="chapter" data-level="6.11" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#create-visualization-of-judge-agreement"><i class="fa fa-check"></i><b>6.11</b> Create visualization of judge agreement</a></li>
<li class="chapter" data-level="6.12" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#item-level-summary"><i class="fa fa-check"></i><b>6.12</b> Item-level summary</a></li>
<li class="chapter" data-level="6.13" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#mathematical-theory-of-factor-analysis-in-validity"><i class="fa fa-check"></i><b>6.13</b> Mathematical Theory of Factor Analysis in Validity</a>
<ul>
<li class="chapter" data-level="6.13.1" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#theoretical-framework-for-factor-based-validity"><i class="fa fa-check"></i><b>6.13.1</b> Theoretical Framework for Factor-Based Validity</a></li>
</ul></li>
<li class="chapter" data-level="6.14" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#summary-and-integration"><i class="fa fa-check"></i><b>6.14</b> Summary and Integration</a>
<ul>
<li class="chapter" data-level="6.14.1" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#future-directions"><i class="fa fa-check"></i><b>6.14.1</b> Future Directions</a></li>
</ul></li>
<li class="chapter" data-level="6.15" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#selection-and-classification-with-multiple-predictors"><i class="fa fa-check"></i><b>6.15</b> Selection and Classification with Multiple Predictors</a>
<ul>
<li class="chapter" data-level="6.15.1" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#multiple-regression-approaches"><i class="fa fa-check"></i><b>6.15.1</b> Multiple Regression Approaches</a></li>
<li class="chapter" data-level="6.15.2" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#cross-validation-and-shrinkage"><i class="fa fa-check"></i><b>6.15.2</b> Cross-Validation and Shrinkage</a></li>
</ul></li>
<li class="chapter" data-level="6.16" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#validation-and-irt"><i class="fa fa-check"></i><b>6.16</b> Validation and IRT</a>
<ul>
<li class="chapter" data-level="6.16.1" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#dif-as-a-validity-threat"><i class="fa fa-check"></i><b>6.16.1</b> DIF as a Validity Threat</a></li>
<li class="chapter" data-level="6.16.2" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#irt-based-validity-evidence"><i class="fa fa-check"></i><b>6.16.2</b> IRT-Based Validity Evidence</a></li>
</ul></li>
<li class="chapter" data-level="6.17" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#conclusion-2"><i class="fa fa-check"></i><b>6.17</b> Conclusion</a></li>
<li class="chapter" data-level="6.18" data-path="validity-and-validation-tests.html"><a href="validity-and-validation-tests.html#references-4"><i class="fa fa-check"></i><b>6.18</b> References</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html"><i class="fa fa-check"></i><b>7</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="7.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#the-mathematical-foundations-of-pca"><i class="fa fa-check"></i><b>7.1</b> The Mathematical Foundations of PCA</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#data-representation-the-data-matrix"><i class="fa fa-check"></i><b>7.1.1</b> Data Representation: The Data Matrix</a></li>
<li class="chapter" data-level="7.1.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#centering-the-data"><i class="fa fa-check"></i><b>7.1.2</b> Centering the Data</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#core-concepts-variance-covariance-and-the-covariance-matrix"><i class="fa fa-check"></i><b>7.2</b> Core Concepts: Variance, Covariance, and the Covariance Matrix</a></li>
<li class="chapter" data-level="7.3" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#the-goal-of-pca-finding-new-bases"><i class="fa fa-check"></i><b>7.3</b> The Goal of PCA: Finding New Bases</a></li>
<li class="chapter" data-level="7.4" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#eigenvectors-and-eigenvalues-the-heart-of-pca"><i class="fa fa-check"></i><b>7.4</b> Eigenvectors and Eigenvalues: The Heart of PCA</a></li>
<li class="chapter" data-level="7.5" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#geometric-interpretation"><i class="fa fa-check"></i><b>7.5</b> Geometric Interpretation</a></li>
<li class="chapter" data-level="7.6" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#steps-in-performing-pca-mathematical-summary"><i class="fa fa-check"></i><b>7.6</b> Steps in Performing PCA (Mathematical Summary)</a></li>
<li class="chapter" data-level="7.7" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#interpreting-pca-results"><i class="fa fa-check"></i><b>7.7</b> Interpreting PCA Results</a></li>
<li class="chapter" data-level="7.8" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#practical-example-of-pca-in-r"><i class="fa fa-check"></i><b>7.8</b> Practical Example of PCA in R</a></li>
<li class="chapter" data-level="7.9" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#performing-pca"><i class="fa fa-check"></i><b>7.9</b> Performing PCA</a></li>
<li class="chapter" data-level="7.10" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#scree-plot-deciding-on-the-number-of-components"><i class="fa fa-check"></i><b>7.10</b> Scree Plot: Deciding on the Number of Components</a></li>
<li class="chapter" data-level="7.11" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#loadings-interpreting-the-components"><i class="fa fa-check"></i><b>7.11</b> Loadings: Interpreting the Components</a></li>
<li class="chapter" data-level="7.12" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#component-scores"><i class="fa fa-check"></i><b>7.12</b> Component Scores</a></li>
<li class="chapter" data-level="7.13" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#assumptions-and-limitations-of-pca"><i class="fa fa-check"></i><b>7.13</b> Assumptions and Limitations of PCA</a></li>
<li class="chapter" data-level="7.14" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#pca-vs.-factor-analysis-fa"><i class="fa fa-check"></i><b>7.14</b> PCA vs. Factor Analysis (FA)</a></li>
<li class="chapter" data-level="7.15" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#conclusion-3"><i class="fa fa-check"></i><b>7.15</b> Conclusion</a></li>
<li class="chapter" data-level="7.16" data-path="principal-component-analysis.html"><a href="principal-component-analysis.html#references-5"><i class="fa fa-check"></i><b>7.16</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chi-square.html"><a href="chi-square.html"><i class="fa fa-check"></i><b>8</b> Understanding Chi-Square Tests: A Foundation for Categorical Data Analysis in Psychology</a>
<ul>
<li class="chapter" data-level="8.1" data-path="chi-square.html"><a href="chi-square.html#what-are-chi-square-tests"><i class="fa fa-check"></i><b>8.1</b> What Are Chi-Square Tests?</a></li>
<li class="chapter" data-level="8.2" data-path="chi-square.html"><a href="chi-square.html#the-three-main-types-of-chi-square-tests"><i class="fa fa-check"></i><b>8.2</b> The Three Main Types of Chi-Square Tests</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="chi-square.html"><a href="chi-square.html#chi-square-goodness-of-fit-test-testing-single-variable-distributions"><i class="fa fa-check"></i><b>8.2.1</b> Chi-Square Goodness-of-Fit Test: Testing Single Variable Distributions</a></li>
<li class="chapter" data-level="8.2.2" data-path="chi-square.html"><a href="chi-square.html#chi-square-test-of-independence-exploring-relationships-between-variables"><i class="fa fa-check"></i><b>8.2.2</b> Chi-Square Test of Independence: Exploring Relationships Between Variables</a></li>
<li class="chapter" data-level="8.2.3" data-path="chi-square.html"><a href="chi-square.html#chi-square-test-of-homogeneity-comparing-distributions-across-groups"><i class="fa fa-check"></i><b>8.2.3</b> Chi-Square Test of Homogeneity: Comparing Distributions Across Groups</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="chi-square.html"><a href="chi-square.html#practical-example-goodness-of-fit-test-in-personality-research"><i class="fa fa-check"></i><b>8.3</b> Practical Example: Goodness-of-Fit Test in Personality Research</a></li>
<li class="chapter" data-level="8.4" data-path="chi-square.html"><a href="chi-square.html#practical-example-test-of-independence-in-therapy-research"><i class="fa fa-check"></i><b>8.4</b> Practical Example: Test of Independence in Therapy Research</a></li>
<li class="chapter" data-level="8.5" data-path="chi-square.html"><a href="chi-square.html#understanding-assumptions-and-requirements"><i class="fa fa-check"></i><b>8.5</b> Understanding Assumptions and Requirements</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="chi-square.html"><a href="chi-square.html#critical-assumptions-for-valid-chi-square-tests"><i class="fa fa-check"></i><b>8.5.1</b> Critical Assumptions for Valid Chi-Square Tests</a></li>
<li class="chapter" data-level="8.5.2" data-path="chi-square.html"><a href="chi-square.html#sample-size-considerations-and-power"><i class="fa fa-check"></i><b>8.5.2</b> Sample Size Considerations and Power</a></li>
<li class="chapter" data-level="8.5.3" data-path="chi-square.html"><a href="chi-square.html#yates-continuity-correction-for-small-samples"><i class="fa fa-check"></i><b>8.5.3</b> Yates’ Continuity Correction for Small Samples</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="chi-square.html"><a href="chi-square.html#advanced-applications-mcnemars-test-for-paired-data"><i class="fa fa-check"></i><b>8.6</b> Advanced Applications: McNemar’s Test for Paired Data</a></li>
<li class="chapter" data-level="8.7" data-path="chi-square.html"><a href="chi-square.html#effect-size-measures-understanding-practical-significance"><i class="fa fa-check"></i><b>8.7</b> Effect Size Measures: Understanding Practical Significance</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="principal_component_analysis.html"><a href="#phi-coefficient-%CF%86-for-22-tables"><i class="fa fa-check"></i><b>8.7.1</b> Phi Coefficient (φ) for 2×2 Tables</a></li>
<li class="chapter" data-level="8.7.2" data-path="chi-square.html"><a href="chi-square.html#cramers-v-for-larger-tables"><i class="fa fa-check"></i><b>8.7.2</b> Cramer’s V for Larger Tables</a></li>
<li class="chapter" data-level="8.7.3" data-path="chi-square.html"><a href="chi-square.html#contingency-coefficient-c"><i class="fa fa-check"></i><b>8.7.3</b> Contingency Coefficient (C)</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="chi-square.html"><a href="chi-square.html#psychological-applications-across-domains"><i class="fa fa-check"></i><b>8.8</b> Psychological Applications Across Domains</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="chi-square.html"><a href="chi-square.html#clinical-psychology-applications-1"><i class="fa fa-check"></i><b>8.8.1</b> Clinical Psychology Applications</a></li>
<li class="chapter" data-level="8.8.2" data-path="chi-square.html"><a href="chi-square.html#cognitive-psychology-applications-1"><i class="fa fa-check"></i><b>8.8.2</b> Cognitive Psychology Applications</a></li>
<li class="chapter" data-level="8.8.3" data-path="chi-square.html"><a href="chi-square.html#behavioral-psychology-applications-1"><i class="fa fa-check"></i><b>8.8.3</b> Behavioral Psychology Applications</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="chi-square.html"><a href="chi-square.html#alternative-tests-and-when-to-use-them"><i class="fa fa-check"></i><b>8.9</b> Alternative Tests and When to Use Them</a>
<ul>
<li class="chapter" data-level="8.9.1" data-path="chi-square.html"><a href="chi-square.html#fishers-exact-test-for-small-samples"><i class="fa fa-check"></i><b>8.9.1</b> Fisher’s Exact Test for Small Samples</a></li>
<li class="chapter" data-level="8.9.2" data-path="chi-square.html"><a href="chi-square.html#likelihood-ratio-chi-square"><i class="fa fa-check"></i><b>8.9.2</b> Likelihood Ratio Chi-Square</a></li>
</ul></li>
<li class="chapter" data-level="8.10" data-path="chi-square.html"><a href="chi-square.html#limitations-and-considerations"><i class="fa fa-check"></i><b>8.10</b> Limitations and Considerations</a>
<ul>
<li class="chapter" data-level="8.10.1" data-path="chi-square.html"><a href="chi-square.html#information-loss-through-categorization"><i class="fa fa-check"></i><b>8.10.1</b> Information Loss Through Categorization</a></li>
<li class="chapter" data-level="8.10.2" data-path="chi-square.html"><a href="chi-square.html#sample-size-sensitivity"><i class="fa fa-check"></i><b>8.10.2</b> Sample Size Sensitivity</a></li>
<li class="chapter" data-level="8.10.3" data-path="chi-square.html"><a href="chi-square.html#post-hoc-analysis-limitations"><i class="fa fa-check"></i><b>8.10.3</b> Post-Hoc Analysis Limitations</a></li>
<li class="chapter" data-level="8.10.4" data-path="chi-square.html"><a href="chi-square.html#assumption-violations"><i class="fa fa-check"></i><b>8.10.4</b> Assumption Violations</a></li>
</ul></li>
<li class="chapter" data-level="8.11" data-path="chi-square.html"><a href="chi-square.html#references-6"><i class="fa fa-check"></i><b>8.11</b> References</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="frequency-distributions.html"><a href="frequency-distributions.html"><i class="fa fa-check"></i><b>9</b> Making Sense of Data Patterns: Understanding Frequency Distributions in Psychology</a>
<ul>
<li class="chapter" data-level="9.1" data-path="frequency-distributions.html"><a href="frequency-distributions.html#what-are-frequency-distributions"><i class="fa fa-check"></i><b>9.1</b> What Are Frequency Distributions?</a></li>
<li class="chapter" data-level="9.2" data-path="frequency-distributions.html"><a href="frequency-distributions.html#the-four-essential-types-of-frequency-distributions"><i class="fa fa-check"></i><b>9.2</b> The Four Essential Types of Frequency Distributions</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="frequency-distributions.html"><a href="frequency-distributions.html#simple-frequency-distribution-the-basic-building-block"><i class="fa fa-check"></i><b>9.2.1</b> Simple Frequency Distribution: The Basic Building Block</a></li>
<li class="chapter" data-level="9.2.2" data-path="frequency-distributions.html"><a href="frequency-distributions.html#cumulative-frequency-distribution-running-totals"><i class="fa fa-check"></i><b>9.2.2</b> Cumulative Frequency Distribution: Running Totals</a></li>
<li class="chapter" data-level="9.2.3" data-path="frequency-distributions.html"><a href="frequency-distributions.html#relative-frequency-distribution-understanding-proportions"><i class="fa fa-check"></i><b>9.2.3</b> Relative Frequency Distribution: Understanding Proportions</a></li>
<li class="chapter" data-level="9.2.4" data-path="frequency-distributions.html"><a href="frequency-distributions.html#grouped-frequency-distribution-taming-large-datasets"><i class="fa fa-check"></i><b>9.2.4</b> Grouped Frequency Distribution: Taming Large Datasets</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="frequency-distributions.html"><a href="frequency-distributions.html#real-world-example-depression-screening-in-clinical-practice"><i class="fa fa-check"></i><b>9.3</b> Real-World Example: Depression Screening in Clinical Practice</a></li>
<li class="chapter" data-level="9.4" data-path="frequency-distributions.html"><a href="frequency-distributions.html#understanding-response-patterns-in-psychological-assessment"><i class="fa fa-check"></i><b>9.4</b> Understanding Response Patterns in Psychological Assessment</a></li>
<li class="chapter" data-level="9.5" data-path="frequency-distributions.html"><a href="frequency-distributions.html#visual-representations-bringing-data-to-life"><i class="fa fa-check"></i><b>9.5</b> Visual Representations: Bringing Data to Life</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="frequency-distributions.html"><a href="frequency-distributions.html#histograms-revealing-the-shape-of-continuous-data"><i class="fa fa-check"></i><b>9.5.1</b> Histograms: Revealing the Shape of Continuous Data</a></li>
<li class="chapter" data-level="9.5.2" data-path="frequency-distributions.html"><a href="frequency-distributions.html#bar-charts-perfect-for-categorical-data"><i class="fa fa-check"></i><b>9.5.2</b> Bar Charts: Perfect for Categorical Data</a></li>
<li class="chapter" data-level="9.5.3" data-path="frequency-distributions.html"><a href="frequency-distributions.html#frequency-polygons-smooth-curves-for-comparison"><i class="fa fa-check"></i><b>9.5.3</b> Frequency Polygons: Smooth Curves for Comparison</a></li>
<li class="chapter" data-level="9.5.4" data-path="frequency-distributions.html"><a href="frequency-distributions.html#ogives-cumulative-frequency-curves"><i class="fa fa-check"></i><b>9.5.4</b> Ogives: Cumulative Frequency Curves</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="frequency-distributions.html"><a href="frequency-distributions.html#statistical-measures-quantifying-distribution-characteristics"><i class="fa fa-check"></i><b>9.6</b> Statistical Measures: Quantifying Distribution Characteristics</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="frequency-distributions.html"><a href="frequency-distributions.html#measures-of-central-tendency-finding-the-center"><i class="fa fa-check"></i><b>9.6.1</b> Measures of Central Tendency: Finding the Center</a></li>
<li class="chapter" data-level="9.6.2" data-path="frequency-distributions.html"><a href="frequency-distributions.html#measures-of-variability-understanding-spread"><i class="fa fa-check"></i><b>9.6.2</b> Measures of Variability: Understanding Spread</a></li>
<li class="chapter" data-level="9.6.3" data-path="frequency-distributions.html"><a href="frequency-distributions.html#measures-of-shape-understanding-distribution-form"><i class="fa fa-check"></i><b>9.6.3</b> Measures of Shape: Understanding Distribution Form</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="frequency-distributions.html"><a href="frequency-distributions.html#practical-applications-in-different-psychology-domains"><i class="fa fa-check"></i><b>9.7</b> Practical Applications in Different Psychology Domains</a>
<ul>
<li class="chapter" data-level="9.7.1" data-path="frequency-distributions.html"><a href="frequency-distributions.html#clinical-psychology-diagnostic-and-treatment-applications"><i class="fa fa-check"></i><b>9.7.1</b> Clinical Psychology: Diagnostic and Treatment Applications</a></li>
<li class="chapter" data-level="9.7.2" data-path="frequency-distributions.html"><a href="frequency-distributions.html#cognitive-psychology-understanding-mental-processes"><i class="fa fa-check"></i><b>9.7.2</b> Cognitive Psychology: Understanding Mental Processes</a></li>
<li class="chapter" data-level="9.7.3" data-path="frequency-distributions.html"><a href="frequency-distributions.html#behavioral-psychology-analyzing-behavior-patterns"><i class="fa fa-check"></i><b>9.7.3</b> Behavioral Psychology: Analyzing Behavior Patterns</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="frequency-distributions.html"><a href="frequency-distributions.html#understanding-distribution-shapes-and-their-psychological-meaning"><i class="fa fa-check"></i><b>9.8</b> Understanding Distribution Shapes and Their Psychological Meaning</a>
<ul>
<li class="chapter" data-level="9.8.1" data-path="frequency-distributions.html"><a href="frequency-distributions.html#normal-distribution-the-gold-standard"><i class="fa fa-check"></i><b>9.8.1</b> Normal Distribution: The Gold Standard</a></li>
<li class="chapter" data-level="9.8.2" data-path="frequency-distributions.html"><a href="frequency-distributions.html#skewed-distributions-when-data-leans-one-way"><i class="fa fa-check"></i><b>9.8.2</b> Skewed Distributions: When Data Leans One Way</a></li>
<li class="chapter" data-level="9.8.3" data-path="frequency-distributions.html"><a href="frequency-distributions.html#bimodal-and-multimodal-distributions-when-groups-hide-in-your-data"><i class="fa fa-check"></i><b>9.8.3</b> Bimodal and Multimodal Distributions: When Groups Hide in Your Data</a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="frequency-distributions.html"><a href="frequency-distributions.html#creating-standardized-scores-from-frequency-distributions"><i class="fa fa-check"></i><b>9.9</b> Creating Standardized Scores from Frequency Distributions</a>
<ul>
<li class="chapter" data-level="9.9.1" data-path="frequency-distributions.html"><a href="frequency-distributions.html#z-scores-the-foundation-of-standardization"><i class="fa fa-check"></i><b>9.9.1</b> Z-Scores: The Foundation of Standardization</a></li>
<li class="chapter" data-level="9.9.2" data-path="frequency-distributions.html"><a href="frequency-distributions.html#t-scores-user-friendly-standardization"><i class="fa fa-check"></i><b>9.9.2</b> T-Scores: User-Friendly Standardization</a></li>
<li class="chapter" data-level="9.9.3" data-path="frequency-distributions.html"><a href="frequency-distributions.html#percentile-ranks-intuitive-comparisons"><i class="fa fa-check"></i><b>9.9.3</b> Percentile Ranks: Intuitive Comparisons</a></li>
</ul></li>
<li class="chapter" data-level="9.10" data-path="frequency-distributions.html"><a href="frequency-distributions.html#applications-in-test-development-and-validation"><i class="fa fa-check"></i><b>9.10</b> Applications in Test Development and Validation</a>
<ul>
<li class="chapter" data-level="9.10.1" data-path="frequency-distributions.html"><a href="frequency-distributions.html#establishing-normative-data"><i class="fa fa-check"></i><b>9.10.1</b> Establishing Normative Data</a></li>
<li class="chapter" data-level="9.10.2" data-path="frequency-distributions.html"><a href="frequency-distributions.html#item-analysis-and-test-refinement"><i class="fa fa-check"></i><b>9.10.2</b> Item Analysis and Test Refinement</a></li>
<li class="chapter" data-level="9.10.3" data-path="frequency-distributions.html"><a href="frequency-distributions.html#diagnostic-decision-making"><i class="fa fa-check"></i><b>9.10.3</b> Diagnostic Decision Making</a></li>
</ul></li>
<li class="chapter" data-level="9.11" data-path="frequency-distributions.html"><a href="frequency-distributions.html#common-challenges-and-solutions-in-frequency-distribution-analysis"><i class="fa fa-check"></i><b>9.11</b> Common Challenges and Solutions in Frequency Distribution Analysis</a>
<ul>
<li class="chapter" data-level="9.11.1" data-path="frequency-distributions.html"><a href="frequency-distributions.html#dealing-with-extreme-scores-and-outliers"><i class="fa fa-check"></i><b>9.11.1</b> Dealing with Extreme Scores and Outliers</a></li>
<li class="chapter" data-level="9.11.2" data-path="frequency-distributions.html"><a href="frequency-distributions.html#handling-missing-data"><i class="fa fa-check"></i><b>9.11.2</b> Handling Missing Data</a></li>
<li class="chapter" data-level="9.11.3" data-path="frequency-distributions.html"><a href="frequency-distributions.html#sample-size-considerations"><i class="fa fa-check"></i><b>9.11.3</b> Sample Size Considerations</a></li>
</ul></li>
<li class="chapter" data-level="9.12" data-path="frequency-distributions.html"><a href="frequency-distributions.html#advanced-applications-and-extensions"><i class="fa fa-check"></i><b>9.12</b> Advanced Applications and Extensions</a>
<ul>
<li class="chapter" data-level="9.12.1" data-path="frequency-distributions.html"><a href="frequency-distributions.html#comparing-multiple-distributions"><i class="fa fa-check"></i><b>9.12.1</b> Comparing Multiple Distributions</a></li>
<li class="chapter" data-level="9.12.2" data-path="frequency-distributions.html"><a href="frequency-distributions.html#mixture-distribution-analysis"><i class="fa fa-check"></i><b>9.12.2</b> Mixture Distribution Analysis</a></li>
<li class="chapter" data-level="9.12.3" data-path="frequency-distributions.html"><a href="frequency-distributions.html#time-series-of-frequency-distributions"><i class="fa fa-check"></i><b>9.12.3</b> Time Series of Frequency Distributions</a></li>
</ul></li>
<li class="chapter" data-level="9.13" data-path="frequency-distributions.html"><a href="frequency-distributions.html#conclusion-the-foundation-of-psychological-measurement"><i class="fa fa-check"></i><b>9.13</b> Conclusion: The Foundation of Psychological Measurement</a></li>
<li class="chapter" data-level="9.14" data-path="frequency-distributions.html"><a href="frequency-distributions.html#references-7"><i class="fa fa-check"></i><b>9.14</b> References</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="information-entropy.html"><a href="information-entropy.html"><i class="fa fa-check"></i><b>10</b> Uncovering Hidden Patterns: Information Entropy in Psychological Measurement</a>
<ul>
<li class="chapter" data-level="10.1" data-path="information-entropy.html"><a href="information-entropy.html#what-is-information-entropy-and-why-should-psychologists-care"><i class="fa fa-check"></i><b>10.1</b> What Is Information Entropy and Why Should Psychologists Care?</a></li>
<li class="chapter" data-level="10.2" data-path="information-entropy.html"><a href="information-entropy.html#the-essential-properties-that-make-entropy-useful"><i class="fa fa-check"></i><b>10.2</b> The Essential Properties That Make Entropy Useful</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="information-entropy.html"><a href="information-entropy.html#non-negativity-information-is-never-negative"><i class="fa fa-check"></i><b>10.2.1</b> Non-Negativity: Information Is Never Negative</a></li>
<li class="chapter" data-level="10.2.2" data-path="information-entropy.html"><a href="information-entropy.html#maximum-entropy-the-point-of-greatest-uncertainty"><i class="fa fa-check"></i><b>10.2.2</b> Maximum Entropy: The Point of Greatest Uncertainty</a></li>
<li class="chapter" data-level="10.2.3" data-path="information-entropy.html"><a href="information-entropy.html#additivity-independent-information-combines-simply"><i class="fa fa-check"></i><b>10.2.3</b> Additivity: Independent Information Combines Simply</a></li>
<li class="chapter" data-level="10.2.4" data-path="information-entropy.html"><a href="information-entropy.html#conditional-entropy-information-after-learning"><i class="fa fa-check"></i><b>10.2.4</b> Conditional Entropy: Information After Learning</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="information-entropy.html"><a href="information-entropy.html#real-world-application-diagnostic-classification-in-clinical-practice"><i class="fa fa-check"></i><b>10.3</b> Real-World Application: Diagnostic Classification in Clinical Practice</a></li>
<li class="chapter" data-level="10.4" data-path="information-entropy.html"><a href="information-entropy.html#understanding-information-gain-in-psychological-assessment"><i class="fa fa-check"></i><b>10.4</b> Understanding Information Gain in Psychological Assessment</a></li>
<li class="chapter" data-level="10.5" data-path="information-entropy.html"><a href="information-entropy.html#entropy-in-test-construction-and-item-analysis"><i class="fa fa-check"></i><b>10.5</b> Entropy in Test Construction and Item Analysis</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="information-entropy.html"><a href="information-entropy.html#identifying-informative-vs.-non-informative-items"><i class="fa fa-check"></i><b>10.5.1</b> Identifying Informative vs. Non-Informative Items</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="information-entropy.html"><a href="information-entropy.html#response-pattern-analysis-detecting-problematic-responding"><i class="fa fa-check"></i><b>10.6</b> Response Pattern Analysis: Detecting Problematic Responding</a></li>
<li class="chapter" data-level="10.7" data-path="information-entropy.html"><a href="information-entropy.html#advanced-applications-in-psychological-research"><i class="fa fa-check"></i><b>10.7</b> Advanced Applications in Psychological Research</a>
<ul>
<li class="chapter" data-level="10.7.1" data-path="information-entropy.html"><a href="information-entropy.html#measuring-construct-complexity-through-factor-analysis"><i class="fa fa-check"></i><b>10.7.1</b> Measuring Construct Complexity Through Factor Analysis</a></li>
<li class="chapter" data-level="10.7.2" data-path="information-entropy.html"><a href="information-entropy.html#mutual-information-measuring-variable-relationships"><i class="fa fa-check"></i><b>10.7.2</b> Mutual Information: Measuring Variable Relationships</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="information-entropy.html"><a href="information-entropy.html#practical-applications-across-psychology-domains"><i class="fa fa-check"></i><b>10.8</b> Practical Applications Across Psychology Domains</a>
<ul>
<li class="chapter" data-level="10.8.1" data-path="information-entropy.html"><a href="information-entropy.html#clinical-psychology-treatment-outcome-prediction"><i class="fa fa-check"></i><b>10.8.1</b> Clinical Psychology: Treatment Outcome Prediction</a></li>
<li class="chapter" data-level="10.8.2" data-path="information-entropy.html"><a href="information-entropy.html#cognitive-psychology-decision-making-analysis"><i class="fa fa-check"></i><b>10.8.2</b> Cognitive Psychology: Decision-Making Analysis</a></li>
<li class="chapter" data-level="10.8.3" data-path="information-entropy.html"><a href="information-entropy.html#behavioral-psychology-response-variability-analysis"><i class="fa fa-check"></i><b>10.8.3</b> Behavioral Psychology: Response Variability Analysis</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="information-entropy.html"><a href="information-entropy.html#advanced-entropy-measures-and-applications"><i class="fa fa-check"></i><b>10.9</b> Advanced Entropy Measures and Applications</a>
<ul>
<li class="chapter" data-level="10.9.1" data-path="information-entropy.html"><a href="information-entropy.html#differential-entropy-for-continuous-variables"><i class="fa fa-check"></i><b>10.9.1</b> Differential Entropy for Continuous Variables</a></li>
<li class="chapter" data-level="10.9.2" data-path="information-entropy.html"><a href="information-entropy.html#kullback-leibler-divergence-measuring-distribution-differences"><i class="fa fa-check"></i><b>10.9.2</b> Kullback-Leibler Divergence: Measuring Distribution Differences</a></li>
<li class="chapter" data-level="10.9.3" data-path="information-entropy.html"><a href="information-entropy.html#jensen-shannon-divergence-symmetric-distribution-comparison"><i class="fa fa-check"></i><b>10.9.3</b> Jensen-Shannon Divergence: Symmetric Distribution Comparison</a></li>
</ul></li>
<li class="chapter" data-level="10.10" data-path="information-entropy.html"><a href="information-entropy.html#emerging-applications-and-future-directions"><i class="fa fa-check"></i><b>10.10</b> Emerging Applications and Future Directions</a>
<ul>
<li class="chapter" data-level="10.10.1" data-path="information-entropy.html"><a href="information-entropy.html#machine-learning-integration"><i class="fa fa-check"></i><b>10.10.1</b> Machine Learning Integration</a></li>
<li class="chapter" data-level="10.10.2" data-path="information-entropy.html"><a href="information-entropy.html#network-analysis-in-psychology"><i class="fa fa-check"></i><b>10.10.2</b> Network Analysis in Psychology</a></li>
<li class="chapter" data-level="10.10.3" data-path="information-entropy.html"><a href="information-entropy.html#personalized-assessment"><i class="fa fa-check"></i><b>10.10.3</b> Personalized Assessment</a></li>
</ul></li>
<li class="chapter" data-level="10.11" data-path="information-entropy.html"><a href="information-entropy.html#practical-guidelines-for-using-entropy-in-psychological-research"><i class="fa fa-check"></i><b>10.11</b> Practical Guidelines for Using Entropy in Psychological Research</a>
<ul>
<li class="chapter" data-level="10.11.1" data-path="information-entropy.html"><a href="information-entropy.html#when-to-use-entropy-measures"><i class="fa fa-check"></i><b>10.11.1</b> When to Use Entropy Measures</a></li>
<li class="chapter" data-level="10.11.2" data-path="information-entropy.html"><a href="information-entropy.html#interpreting-entropy-values"><i class="fa fa-check"></i><b>10.11.2</b> Interpreting Entropy Values</a></li>
<li class="chapter" data-level="10.11.3" data-path="information-entropy.html"><a href="information-entropy.html#reporting-entropy-results"><i class="fa fa-check"></i><b>10.11.3</b> Reporting Entropy Results</a></li>
</ul></li>
<li class="chapter" data-level="10.12" data-path="information-entropy.html"><a href="information-entropy.html#limitations-and-considerations-1"><i class="fa fa-check"></i><b>10.12</b> Limitations and Considerations</a>
<ul>
<li class="chapter" data-level="10.12.1" data-path="information-entropy.html"><a href="information-entropy.html#sample-size-requirements"><i class="fa fa-check"></i><b>10.12.1</b> Sample Size Requirements</a></li>
<li class="chapter" data-level="10.12.2" data-path="information-entropy.html"><a href="information-entropy.html#independence-assumptions"><i class="fa fa-check"></i><b>10.12.2</b> Independence Assumptions</a></li>
<li class="chapter" data-level="10.12.3" data-path="information-entropy.html"><a href="information-entropy.html#cultural-and-linguistic-considerations"><i class="fa fa-check"></i><b>10.12.3</b> Cultural and Linguistic Considerations</a></li>
</ul></li>
<li class="chapter" data-level="10.13" data-path="information-entropy.html"><a href="information-entropy.html#integration-with-traditional-psychometric-approaches"><i class="fa fa-check"></i><b>10.13</b> Integration with Traditional Psychometric Approaches</a>
<ul>
<li class="chapter" data-level="10.13.1" data-path="information-entropy.html"><a href="information-entropy.html#complementing-classical-test-theory"><i class="fa fa-check"></i><b>10.13.1</b> Complementing Classical Test Theory</a></li>
<li class="chapter" data-level="10.13.2" data-path="information-entropy.html"><a href="information-entropy.html#enhancing-item-response-theory"><i class="fa fa-check"></i><b>10.13.2</b> Enhancing Item Response Theory</a></li>
</ul></li>
<li class="chapter" data-level="10.14" data-path="information-entropy.html"><a href="information-entropy.html#conclusion-the-information-revolution-in-psychological-measurement"><i class="fa fa-check"></i><b>10.14</b> Conclusion: The Information Revolution in Psychological Measurement</a></li>
<li class="chapter" data-level="10.15" data-path="information-entropy.html"><a href="information-entropy.html#references-8"><i class="fa fa-check"></i><b>10.15</b> References</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="mode.html"><a href="mode.html"><i class="fa fa-check"></i><b>11</b> Understanding the Mode: Finding the Most Common Values in Psychological Data</a>
<ul>
<li class="chapter" data-level="11.1" data-path="mode.html"><a href="mode.html#what-is-the-mode-and-why-does-it-matter-in-psychology"><i class="fa fa-check"></i><b>11.1</b> What Is the Mode and Why Does It Matter in Psychology?</a></li>
<li class="chapter" data-level="11.2" data-path="mode.html"><a href="mode.html#essential-properties-that-make-the-mode-unique"><i class="fa fa-check"></i><b>11.2</b> Essential Properties That Make the Mode Unique</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="mode.html"><a href="mode.html#non-uniqueness-multiple-peaks-are-possible"><i class="fa fa-check"></i><b>11.2.1</b> Non-Uniqueness: Multiple Peaks Are Possible</a></li>
<li class="chapter" data-level="11.2.2" data-path="mode.html"><a href="mode.html#resistance-to-extreme-values"><i class="fa fa-check"></i><b>11.2.2</b> Resistance to Extreme Values</a></li>
<li class="chapter" data-level="11.2.3" data-path="mode.html"><a href="mode.html#universal-applicability-across-measurement-scales"><i class="fa fa-check"></i><b>11.2.3</b> Universal Applicability Across Measurement Scales</a></li>
<li class="chapter" data-level="11.2.4" data-path="mode.html"><a href="mode.html#lack-of-algebraic-properties"><i class="fa fa-check"></i><b>11.2.4</b> Lack of Algebraic Properties</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="mode.html"><a href="mode.html#real-world-application-diagnostic-categories-in-clinical-practice"><i class="fa fa-check"></i><b>11.3</b> Real-World Application: Diagnostic Categories in Clinical Practice</a></li>
<li class="chapter" data-level="11.4" data-path="mode.html"><a href="mode.html#understanding-response-patterns-in-psychological-assessment-1"><i class="fa fa-check"></i><b>11.4</b> Understanding Response Patterns in Psychological Assessment</a></li>
<li class="chapter" data-level="11.5" data-path="mode.html"><a href="mode.html#calculating-the-mode-from-simple-to-complex"><i class="fa fa-check"></i><b>11.5</b> Calculating the Mode: From Simple to Complex</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="mode.html"><a href="mode.html#simple-mode-calculation-for-categorical-data"><i class="fa fa-check"></i><b>11.5.1</b> Simple Mode Calculation for Categorical Data</a></li>
<li class="chapter" data-level="11.5.2" data-path="mode.html"><a href="mode.html#mode-for-grouped-data-estimation-from-class-intervals"><i class="fa fa-check"></i><b>11.5.2</b> Mode for Grouped Data: Estimation from Class Intervals</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="mode.html"><a href="mode.html#comparing-mode-with-mean-and-median"><i class="fa fa-check"></i><b>11.6</b> Comparing Mode with Mean and Median</a></li>
<li class="chapter" data-level="11.7" data-path="mode.html"><a href="mode.html#advanced-applications-mixture-models-and-bimodal-distributions"><i class="fa fa-check"></i><b>11.7</b> Advanced Applications: Mixture Models and Bimodal Distributions</a></li>
<li class="chapter" data-level="11.8" data-path="mode.html"><a href="mode.html#statistical-inference-and-bootstrap-confidence-intervals"><i class="fa fa-check"></i><b>11.8</b> Statistical Inference and Bootstrap Confidence Intervals</a></li>
<li class="chapter" data-level="11.9" data-path="mode.html"><a href="mode.html#mode-applications-across-psychology-domains"><i class="fa fa-check"></i><b>11.9</b> Mode Applications Across Psychology Domains</a>
<ul>
<li class="chapter" data-level="11.9.1" data-path="mode.html"><a href="mode.html#clinical-psychology-diagnostic-and-treatment-applications-1"><i class="fa fa-check"></i><b>11.9.1</b> Clinical Psychology: Diagnostic and Treatment Applications</a></li>
<li class="chapter" data-level="11.9.2" data-path="mode.html"><a href="mode.html#cognitive-psychology-understanding-mental-processes-1"><i class="fa fa-check"></i><b>11.9.2</b> Cognitive Psychology: Understanding Mental Processes</a></li>
<li class="chapter" data-level="11.9.3" data-path="mode.html"><a href="mode.html#behavioral-psychology-analyzing-action-patterns"><i class="fa fa-check"></i><b>11.9.3</b> Behavioral Psychology: Analyzing Action Patterns</a></li>
</ul></li>
<li class="chapter" data-level="11.10" data-path="mode.html"><a href="mode.html#practical-guidelines-for-using-the-mode"><i class="fa fa-check"></i><b>11.10</b> Practical Guidelines for Using the Mode</a>
<ul>
<li class="chapter" data-level="11.10.1" data-path="mode.html"><a href="mode.html#when-the-mode-is-most-appropriate"><i class="fa fa-check"></i><b>11.10.1</b> When the Mode Is Most Appropriate</a></li>
<li class="chapter" data-level="11.10.2" data-path="mode.html"><a href="mode.html#reporting-mode-results"><i class="fa fa-check"></i><b>11.10.2</b> Reporting Mode Results</a></li>
<li class="chapter" data-level="11.10.3" data-path="mode.html"><a href="mode.html#common-pitfalls-to-avoid"><i class="fa fa-check"></i><b>11.10.3</b> Common Pitfalls to Avoid</a></li>
</ul></li>
<li class="chapter" data-level="11.11" data-path="mode.html"><a href="mode.html#integration-with-modern-statistical-approaches"><i class="fa fa-check"></i><b>11.11</b> Integration with Modern Statistical Approaches</a>
<ul>
<li class="chapter" data-level="11.11.1" data-path="mode.html"><a href="mode.html#machine-learning-applications-1"><i class="fa fa-check"></i><b>11.11.1</b> Machine Learning Applications</a></li>
<li class="chapter" data-level="11.11.2" data-path="mode.html"><a href="mode.html#bayesian-statistics"><i class="fa fa-check"></i><b>11.11.2</b> Bayesian Statistics</a></li>
<li class="chapter" data-level="11.11.3" data-path="mode.html"><a href="mode.html#meta-analysis"><i class="fa fa-check"></i><b>11.11.3</b> Meta-Analysis</a></li>
</ul></li>
<li class="chapter" data-level="11.12" data-path="mode.html"><a href="mode.html#future-directions-and-emerging-applications"><i class="fa fa-check"></i><b>11.12</b> Future Directions and Emerging Applications</a>
<ul>
<li class="chapter" data-level="11.12.1" data-path="mode.html"><a href="mode.html#big-data-psychology"><i class="fa fa-check"></i><b>11.12.1</b> Big Data Psychology</a></li>
<li class="chapter" data-level="11.12.2" data-path="mode.html"><a href="mode.html#personalized-psychology"><i class="fa fa-check"></i><b>11.12.2</b> Personalized Psychology</a></li>
<li class="chapter" data-level="11.12.3" data-path="mode.html"><a href="mode.html#cross-cultural-research"><i class="fa fa-check"></i><b>11.12.3</b> Cross-Cultural Research</a></li>
</ul></li>
<li class="chapter" data-level="11.13" data-path="mode.html"><a href="mode.html#conclusion-the-modes-unique-contribution-to-psychological-understanding"><i class="fa fa-check"></i><b>11.13</b> Conclusion: The Mode’s Unique Contribution to Psychological Understanding</a></li>
<li class="chapter" data-level="11.14" data-path="mode.html"><a href="mode.html#references-9"><i class="fa fa-check"></i><b>11.14</b> References</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="mathematical-functions-in-psychology.html"><a href="mathematical-functions-in-psychology.html"><i class="fa fa-check"></i><b>12</b> Mathematical Functions in Psychology</a>
<ul>
<li class="chapter" data-level="12.1" data-path="mathematical-functions-in-psychology.html"><a href="mathematical-functions-in-psychology.html#introduction-to-functions"><i class="fa fa-check"></i><b>12.1</b> Introduction to Functions</a></li>
<li class="chapter" data-level="12.2" data-path="mathematical-functions-in-psychology.html"><a href="mathematical-functions-in-psychology.html#basic-types-of-functions-relevant-to-psychology"><i class="fa fa-check"></i><b>12.2</b> Basic Types of Functions Relevant to Psychology</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="mathematical-functions-in-psychology.html"><a href="mathematical-functions-in-psychology.html#linear-functions"><i class="fa fa-check"></i><b>12.2.1</b> Linear Functions</a></li>
<li class="chapter" data-level="12.2.2" data-path="mathematical-functions-in-psychology.html"><a href="mathematical-functions-in-psychology.html#exponential-functions"><i class="fa fa-check"></i><b>12.2.2</b> Exponential Functions</a></li>
<li class="chapter" data-level="12.2.3" data-path="mathematical-functions-in-psychology.html"><a href="mathematical-functions-in-psychology.html#logarithmic-functions"><i class="fa fa-check"></i><b>12.2.3</b> Logarithmic Functions</a></li>
<li class="chapter" data-level="12.2.4" data-path="mathematical-functions-in-psychology.html"><a href="mathematical-functions-in-psychology.html#power-functions"><i class="fa fa-check"></i><b>12.2.4</b> Power Functions</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="mathematical-functions-in-psychology.html"><a href="mathematical-functions-in-psychology.html#interpreting-functions-in-psychological-research"><i class="fa fa-check"></i><b>12.3</b> Interpreting Functions in Psychological Research</a></li>
<li class="chapter" data-level="12.4" data-path="mathematical-functions-in-psychology.html"><a href="mathematical-functions-in-psychology.html#applications-of-functions-in-different-areas-of-psychology"><i class="fa fa-check"></i><b>12.4</b> Applications of Functions in Different Areas of Psychology</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="mathematical-functions-in-psychology.html"><a href="mathematical-functions-in-psychology.html#clinical-psychology"><i class="fa fa-check"></i><b>12.4.1</b> Clinical Psychology</a></li>
<li class="chapter" data-level="12.4.2" data-path="mathematical-functions-in-psychology.html"><a href="mathematical-functions-in-psychology.html#cognitive-psychology"><i class="fa fa-check"></i><b>12.4.2</b> Cognitive Psychology</a></li>
<li class="chapter" data-level="12.4.3" data-path="mathematical-functions-in-psychology.html"><a href="mathematical-functions-in-psychology.html#developmental-psychology"><i class="fa fa-check"></i><b>12.4.3</b> Developmental Psychology</a></li>
<li class="chapter" data-level="12.4.4" data-path="mathematical-functions-in-psychology.html"><a href="mathematical-functions-in-psychology.html#social-psychology"><i class="fa fa-check"></i><b>12.4.4</b> Social Psychology</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="mathematical-functions-in-psychology.html"><a href="mathematical-functions-in-psychology.html#creating-and-testing-functional-models"><i class="fa fa-check"></i><b>12.5</b> Creating and Testing Functional Models</a></li>
<li class="chapter" data-level="12.6" data-path="mathematical-functions-in-psychology.html"><a href="mathematical-functions-in-psychology.html#statistical-approaches-to-functional-relationships"><i class="fa fa-check"></i><b>12.6</b> Statistical Approaches to Functional Relationships</a></li>
<li class="chapter" data-level="12.7" data-path="mathematical-functions-in-psychology.html"><a href="mathematical-functions-in-psychology.html#conclusion-4"><i class="fa fa-check"></i><b>12.7</b> Conclusion</a></li>
<li class="chapter" data-level="12.8" data-path="mathematical-functions-in-psychology.html"><a href="mathematical-functions-in-psychology.html#references-10"><i class="fa fa-check"></i><b>12.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="math-notation.html"><a href="math-notation.html"><i class="fa fa-check"></i><b>13</b> Mathematical Notation Reference Guide</a>
<ul>
<li class="chapter" data-level="13.1" data-path="math-notation.html"><a href="math-notation.html#greek-letters"><i class="fa fa-check"></i><b>13.1</b> Greek Letters</a></li>
<li class="chapter" data-level="13.2" data-path="math-notation.html"><a href="math-notation.html#subscripts-and-superscripts"><i class="fa fa-check"></i><b>13.2</b> Subscripts and Superscripts</a></li>
<li class="chapter" data-level="13.3" data-path="math-notation.html"><a href="math-notation.html#statistical-notation"><i class="fa fa-check"></i><b>13.3</b> Statistical Notation</a></li>
<li class="chapter" data-level="13.4" data-path="math-notation.html"><a href="math-notation.html#specific-to-classical-test-theory"><i class="fa fa-check"></i><b>13.4</b> Specific to Classical Test Theory</a></li>
<li class="chapter" data-level="13.5" data-path="math-notation.html"><a href="math-notation.html#mathematical-operations"><i class="fa fa-check"></i><b>13.5</b> Mathematical Operations</a></li>
<li class="chapter" data-level="13.6" data-path="math-notation.html"><a href="math-notation.html#set-notation"><i class="fa fa-check"></i><b>13.6</b> Set Notation</a></li>
<li class="chapter" data-level="13.7" data-path="math-notation.html"><a href="math-notation.html#probability-notation"><i class="fa fa-check"></i><b>13.7</b> Probability Notation</a></li>
<li class="chapter" data-level="13.8" data-path="math-notation.html"><a href="math-notation.html#scales-of-measurement"><i class="fa fa-check"></i><b>13.8</b> Scales of Measurement</a>
<ul>
<li class="chapter" data-level="13.8.1" data-path="math-notation.html"><a href="math-notation.html#nominal-scale"><i class="fa fa-check"></i><b>13.8.1</b> Nominal Scale</a></li>
<li class="chapter" data-level="13.8.2" data-path="math-notation.html"><a href="math-notation.html#ordinal-scale"><i class="fa fa-check"></i><b>13.8.2</b> Ordinal Scale</a></li>
<li class="chapter" data-level="13.8.3" data-path="math-notation.html"><a href="math-notation.html#interval-scale"><i class="fa fa-check"></i><b>13.8.3</b> Interval Scale</a></li>
<li class="chapter" data-level="13.8.4" data-path="math-notation.html"><a href="math-notation.html#ratio-scale"><i class="fa fa-check"></i><b>13.8.4</b> Ratio Scale</a></li>
</ul></li>
<li class="chapter" data-level="13.9" data-path="math-notation.html"><a href="math-notation.html#test-theory-formulas"><i class="fa fa-check"></i><b>13.9</b> Test Theory Formulas</a></li>
<li class="chapter" data-level="13.10" data-path="math-notation.html"><a href="math-notation.html#item-response-theory-irt-formulas"><i class="fa fa-check"></i><b>13.10</b> Item Response Theory (IRT) Formulas</a></li>
<li class="chapter" data-level="13.11" data-path="math-notation.html"><a href="math-notation.html#other-important-statistical-formulas"><i class="fa fa-check"></i><b>13.11</b> Other Important Statistical Formulas</a></li>
<li class="chapter" data-level="13.12" data-path="math-notation.html"><a href="math-notation.html#measurement-and-scaling-formulas"><i class="fa fa-check"></i><b>13.12</b> Measurement and Scaling Formulas</a></li>
<li class="chapter" data-level="13.13" data-path="math-notation.html"><a href="math-notation.html#matrix-notation"><i class="fa fa-check"></i><b>13.13</b> Matrix Notation</a></li>
<li class="chapter" data-level="13.14" data-path="math-notation.html"><a href="math-notation.html#multivariate-statistics"><i class="fa fa-check"></i><b>13.14</b> Multivariate Statistics</a></li>
<li class="chapter" data-level="13.15" data-path="math-notation.html"><a href="math-notation.html#how-to-use-this-reference-guide"><i class="fa fa-check"></i><b>13.15</b> How to Use This Reference Guide</a></li>
<li class="chapter" data-level="13.16" data-path="math-notation.html"><a href="math-notation.html#references-and-further-reading"><i class="fa fa-check"></i><b>13.16</b> References and Further Reading</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Principal Component Analysis: Unveiling the Structure of Psychological Data</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="principal-component-analysis" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapter 7</span> Principal Component Analysis<a href="principal-component-analysis.html#principal-component-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Imagine you’re a psychological researcher who has just collected a large amount of data. Perhaps you’ve administered a lengthy questionnaire with dozens, or even hundreds, of items to a group of participants. Each item is a variable, and while each one provides a piece of information, looking at them all individually can be overwhelming. How can you make sense of this complex web of data? How can you see the forest for the trees? This is where Principal Component Analysis (PCA) comes into play.</p>
<p>Principal Component Analysis is a powerful statistical technique used for dimensionality reduction. In simpler terms, it helps to reduce the number of variables in a dataset while trying to preserve as much of the original information (variance) as possible. It achieves this by transforming the original set of possibly correlated variables into a new set of uncorrelated variables, called <strong>principal components (PCs)</strong>. These PCs are ordered such that the first PC captures the largest possible variance in the data, the second PC captures the largest remaining variance (and is uncorrelated with the first), and so on.</p>
<p>The importance of PCA in psychology is vast. It’s frequently used in:
- <strong>Questionnaire Development and Analysis</strong>: To understand the underlying structure of psychological scales. For example, if a new anxiety scale has 20 items, PCA can help determine if these items measure a single dimension of anxiety or multiple facets (e.g., cognitive anxiety, somatic anxiety).
- <strong>Data Simplification</strong>: To reduce a large number of predictors in regression models, which can help prevent overfitting and improve model interpretability.
- <strong>Exploring Interrelations</strong>: To uncover hidden patterns and relationships among a set of observed psychological variables, such as different cognitive abilities or personality traits.
- <strong>Visualization</strong>: To visualize high-dimensional data in a lower-dimensional space (typically 2D or 3D), making it easier to spot clusters or trends.</p>
<p><strong>A Non-Mathematical Example:</strong></p>
<p>Let’s consider a simple, non-mathematical example. Imagine you’re trying to assess the “athletic ability” of a group of individuals. You measure several attributes: their 100-meter sprint time, long jump distance, shot put distance, and high jump height. These are four different variables.</p>
<p>You might find that individuals who are good at sprinting also tend to be good at the long jump (as both require explosive leg power). Similarly, shot put and high jump might also show some correlations with other events or with each other. PCA would take these four correlated variables and try to create new, summary variables (the principal components).</p>
<ul>
<li>The <strong>first principal component (PC1)</strong> might represent a general “explosive power and speed” dimension, as it would be heavily influenced by sprint time and long jump distance, and perhaps to a lesser extent by the others. This PC1 would capture the largest chunk of the differences in athletic ability among the individuals.</li>
<li>The <strong>second principal component (PC2)</strong> might capture something else, like “upper body strength vs. agility,” perhaps contrasting shot put performance with aspects not fully captured by PC1. This PC2 would capture the next largest chunk of variation that wasn’t explained by PC1.</li>
</ul>
<p>Instead of dealing with four separate scores, you might now primarily focus on PC1, or perhaps PC1 and PC2, to get a good summary of an individual’s overall athletic profile, having reduced the complexity of your data. PCA, in essence, helps find the most important underlying dimensions in your data.</p>
<p>In this chapter, we will delve into the mathematical foundations that make PCA work, explore how to conduct PCA using R, and learn how to interpret its results in a psychologically meaningful way. For detailed explanations of mathematical symbols used, please refer to the “Mathematical Notation Reference” chapter.</p>
<div id="the-mathematical-foundations-of-pca" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> The Mathematical Foundations of PCA<a href="principal-component-analysis.html#the-mathematical-foundations-of-pca" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To understand PCA, we first need to grasp a few fundamental mathematical concepts. PCA is fundamentally about understanding the variance structure of your data, which involves concepts from linear algebra and statistics.</p>
<div id="data-representation-the-data-matrix" class="section level3 hasAnchor" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Data Representation: The Data Matrix<a href="principal-component-analysis.html#data-representation-the-data-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s assume our dataset consists of <span class="math inline">\(n\)</span> observations (e.g., participants) and <span class="math inline">\(p\)</span> variables (e.g., questionnaire items). We can represent this data as an <span class="math inline">\(n \times p\)</span> matrix, <span class="math inline">\(\mathbf{X}\)</span>:</p>
<p><span class="math display">\[\mathbf{X} = \begin{pmatrix}
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p} \\
x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{np}
\end{pmatrix}\]</span></p>
<p>Each row <span class="math inline">\(\mathbf{x}_i = (x_{i1}, x_{i2}, ..., x_{ip})\)</span> represents the data for the <span class="math inline">\(i\)</span>-th observation, and each column <span class="math inline">\(\mathbf{X}_j = (x_{1j}, x_{2j}, ..., x_{nj})^T\)</span> represents the data for the <span class="math inline">\(j\)</span>-th variable.</p>
</div>
<div id="centering-the-data" class="section level3 hasAnchor" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Centering the Data<a href="principal-component-analysis.html#centering-the-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before PCA, the data is typically <strong>centered</strong> by subtracting the mean of each variable from all its observations. If <span class="math inline">\(\bar{x}_j = \frac{1}{n}\sum_{i=1}^{n}x_{ij}\)</span> is the mean of the <span class="math inline">\(j\)</span>-th variable, then the centered data <span class="math inline">\(x&#39;_{ij} = x_{ij} - \bar{x}_j\)</span>. Let <span class="math inline">\(\mathbf{X}&#39;\)</span> be the centered data matrix. This step ensures that the first principal component describes the direction of maximum variance around the multivariate mean of the data.</p>
</div>
</div>
<div id="core-concepts-variance-covariance-and-the-covariance-matrix" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Core Concepts: Variance, Covariance, and the Covariance Matrix<a href="principal-component-analysis.html#core-concepts-variance-covariance-and-the-covariance-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p><strong>Variance (<span class="math inline">\(s^2_j\)</span> or <span class="math inline">\(\sigma^2_j\)</span>)</strong>: Variance measures the spread or dispersion of a single variable <span class="math inline">\(X_j\)</span> around its mean <span class="math inline">\(\bar{x}_j\)</span>. For a variable <span class="math inline">\(X_j\)</span> with <span class="math inline">\(n\)</span> observations, the sample variance is:
<span class="math display">\[s^2_j = \frac{\sum_{i=1}^{n}(x_{ij} - \bar{x}_j)^2}{n-1}\]</span>
A higher variance means the data points for that variable are more spread out.</p></li>
<li><p><strong>Covariance (<span class="math inline">\(s_{jk}\)</span> or <span class="math inline">\(\sigma_{jk}\)</span>)</strong>: Covariance measures how two variables, <span class="math inline">\(X_j\)</span> and <span class="math inline">\(X_k\)</span>, change together (co-vary). For two variables <span class="math inline">\(X_j\)</span> and <span class="math inline">\(X_k\)</span> with <span class="math inline">\(n\)</span> observations, the sample covariance is:
<span class="math display">\[s_{jk} = \frac{\sum_{i=1}^{n}(x_{ij} - \bar{x}_j)(x_{ik} - \bar{x}_k)}{n-1}\]</span></p>
<ul>
<li>A positive covariance (<span class="math inline">\(s_{jk} &gt; 0\)</span>) indicates that as <span class="math inline">\(X_j\)</span> increases, <span class="math inline">\(X_k\)</span> tends to increase.</li>
<li>A negative covariance (<span class="math inline">\(s_{jk} &lt; 0\)</span>) suggests that as <span class="math inline">\(X_j\)</span> increases, <span class="math inline">\(X_k\)</span> tends to decrease.</li>
<li>A covariance near zero (<span class="math inline">\(s_{jk} \approx 0\)</span>) implies little linear relationship between <span class="math inline">\(X_j\)</span> and <span class="math inline">\(X_k\)</span>.
Note that <span class="math inline">\(s_{jj} = s^2_j\)</span>, the variance of <span class="math inline">\(X_j\)</span>.</li>
</ul></li>
<li><p><strong>Covariance Matrix (<span class="math inline">\(\mathbf{S}\)</span> or <span class="math inline">\(\Sigma\)</span>)</strong>: When dealing with <span class="math inline">\(p\)</span> variables, their variances and covariances can be organized into a symmetric <span class="math inline">\(p \times p\)</span> matrix called the sample covariance matrix, <span class="math inline">\(\mathbf{S}\)</span>.
<span class="math display">\[\mathbf{S} = \begin{pmatrix}
s^2_1 &amp; s_{12} &amp; \cdots &amp; s_{1p} \\
s_{21} &amp; s^2_2 &amp; \cdots &amp; s_{2p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
s_{p1} &amp; s_{p2} &amp; \cdots &amp; s^2_p
\end{pmatrix}\]</span>
where <span class="math inline">\(s_{jk} = s_{kj}\)</span>. The covariance matrix can be computed from the centered data matrix <span class="math inline">\(\mathbf{X}&#39;\)</span> as:
<span class="math display">\[\mathbf{S} = \frac{1}{n-1} (\mathbf{X}&#39;)^T \mathbf{X}&#39;\]</span>
This matrix is fundamental to PCA as it summarizes the inter-relationships and variances of all variables. The sum of the diagonal elements of <span class="math inline">\(\mathbf{S}\)</span> (its trace) is the total variance in the dataset: Total Variance = <span class="math inline">\(\sum_{j=1}^{p} s^2_j = tr(\mathbf{S})\)</span>.</p></li>
<li><p><strong>Standardization and the Correlation Matrix (<span class="math inline">\(\mathbf{R}\)</span>)</strong>:
If variables are measured on different scales (e.g., age in years, reaction time in milliseconds), their variances and covariances can be misleading. A variable measured in millimeters will have a much larger variance than the same variable measured in meters, potentially dominating the PCA. To address this, variables are often <strong>standardized</strong> before PCA. Standardization converts each observation <span class="math inline">\(x_{ij}\)</span> to a z-score:
<span class="math display">\[z_{ij} = \frac{x_{ij} - \bar{x}_j}{s_j}\]</span>
where <span class="math inline">\(s_j\)</span> is the standard deviation of variable <span class="math inline">\(X_j\)</span>. Standardized variables have a mean of 0 and a standard deviation (and variance) of 1.
PCA performed on standardized data is equivalent to performing PCA on the <strong>correlation matrix (<span class="math inline">\(\mathbf{R}\)</span>)</strong>. The correlation matrix is a <span class="math inline">\(p \times p\)</span> matrix where the element <span class="math inline">\(r_{jk}\)</span> is the Pearson correlation coefficient between <span class="math inline">\(X_j\)</span> and <span class="math inline">\(X_k\)</span>:
<span class="math display">\[r_{jk} = \frac{s_{jk}}{s_j s_k} = \frac{\sum_{i=1}^{n}(x_{ij} - \bar{x}_j)(x_{ik} - \bar{x}_k)}{(n-1)s_j s_k}\]</span>
The diagonal elements of <span class="math inline">\(\mathbf{R}\)</span> are all 1 (correlation of a variable with itself). The total variance in standardized data is <span class="math inline">\(p\)</span> (the number of variables), since each standardized variable has a variance of 1.</p></li>
</ol>
</div>
<div id="the-goal-of-pca-finding-new-bases" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> The Goal of PCA: Finding New Bases<a href="principal-component-analysis.html#the-goal-of-pca-finding-new-bases" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The primary goal of PCA is to find a new set of <span class="math inline">\(p\)</span> orthogonal (perpendicular) axes, called principal components, in the <span class="math inline">\(p\)</span>-dimensional variable space. These axes are chosen such that:
- The first principal component (<span class="math inline">\(PC_1\)</span>) aligns with the direction of maximum variance in the data.
- The second principal component (<span class="math inline">\(PC_2\)</span>) aligns with the direction of maximum <em>remaining</em> variance, subject to being orthogonal to <span class="math inline">\(PC_1\)</span>.
- This continues for all <span class="math inline">\(p\)</span> components.</p>
<p>Each principal component <span class="math inline">\(PC_j\)</span> is a <strong>linear combination</strong> of the original (centered, and possibly standardized) variables <span class="math inline">\(X&#39;_1, X&#39;_2, ..., X&#39;_p\)</span>:
<span class="math display">\[PC_j = w_{j1}X&#39;_1 + w_{j2}X&#39;_2 + ... + w_{jp}X&#39;_p = \mathbf{w}_j^T \mathbf{X}&#39;\]</span>
Here, <span class="math inline">\(\mathbf{w}_j = (w_{j1}, w_{j2}, ..., w_{jp})^T\)</span> is a vector of <strong>weights</strong> or <strong>loadings</strong> (the terminology can vary; sometimes “loadings” refers to correlations between variables and PCs, as discussed later). These weights define the direction of the <span class="math inline">\(j\)</span>-th principal component. The vectors <span class="math inline">\(\mathbf{w}_j\)</span> are constrained to be unit vectors (i.e., <span class="math inline">\(\mathbf{w}_j^T \mathbf{w}_j = \sum_{k=1}^{p} w_{jk}^2 = 1\)</span>) and orthogonal to each other (i.e., <span class="math inline">\(\mathbf{w}_j^T \mathbf{w}_k = 0\)</span> for <span class="math inline">\(j \neq k\)</span>).</p>
<p>The variance of the <span class="math inline">\(j\)</span>-th principal component, <span class="math inline">\(Var(PC_j)\)</span>, is what we want to maximize sequentially.</p>
</div>
<div id="eigenvectors-and-eigenvalues-the-heart-of-pca" class="section level2 hasAnchor" number="7.4">
<h2><span class="header-section-number">7.4</span> Eigenvectors and Eigenvalues: The Heart of PCA<a href="principal-component-analysis.html#eigenvectors-and-eigenvalues-the-heart-of-pca" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The mathematical solution to finding these principal components involves the <strong>eigendecomposition</strong> of the covariance matrix <span class="math inline">\(\mathbf{S}\)</span> (or correlation matrix <span class="math inline">\(\mathbf{R}\)</span> if data was standardized).</p>
<p>For a square <span class="math inline">\(p \times p\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span> (which will be <span class="math inline">\(\mathbf{S}\)</span> or <span class="math inline">\(\mathbf{R}\)</span> in our case), an <strong>eigenvector</strong> <span class="math inline">\(\mathbf{v}\)</span> is a non-zero vector that, when multiplied by <span class="math inline">\(\mathbf{A}\)</span>, results in a vector that is simply a scaled version of the original eigenvector. The <strong>eigenvalue</strong> <span class="math inline">\(\lambda\)</span> is that scalar factor.
<span class="math display">\[\mathbf{A}\mathbf{v} = \lambda\mathbf{v}\]</span></p>
<p>A <span class="math inline">\(p \times p\)</span> symmetric matrix (like <span class="math inline">\(\mathbf{S}\)</span> or <span class="math inline">\(\mathbf{R}\)</span>) will have <span class="math inline">\(p\)</span> real eigenvalues (<span class="math inline">\(\lambda_1, \lambda_2, ..., \lambda_p\)</span>) and <span class="math inline">\(p\)</span> corresponding eigenvectors (<span class="math inline">\(\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_p\)</span>) which can be chosen to be orthonormal (orthogonal and of unit length).</p>
<p>In PCA:
- The <strong>eigenvectors</strong> of <span class="math inline">\(\mathbf{S}\)</span> (or <span class="math inline">\(\mathbf{R}\)</span>) are precisely the weight vectors <span class="math inline">\(\mathbf{w}_j\)</span> that define the principal components. That is, <span class="math inline">\(\mathbf{w}_j = \mathbf{v}_j\)</span>. The <span class="math inline">\(j\)</span>-th eigenvector <span class="math inline">\(\mathbf{v}_j\)</span> gives the direction of the <span class="math inline">\(j\)</span>-th principal component.
- The <strong>eigenvalue</strong> <span class="math inline">\(\lambda_j\)</span> corresponding to eigenvector <span class="math inline">\(\mathbf{v}_j\)</span> is the <strong>variance</strong> of the data along that principal component direction. That is, <span class="math inline">\(Var(PC_j) = \lambda_j\)</span>.</p>
<p>The eigenvectors are typically sorted such that their corresponding eigenvalues are in descending order: <span class="math inline">\(\lambda_1 \ge \lambda_2 \ge ... \ge \lambda_p \ge 0\)</span>.
- <span class="math inline">\(PC_1\)</span> (associated with <span class="math inline">\(\lambda_1\)</span>) is the component that explains the largest amount of variance.
- <span class="math inline">\(PC_2\)</span> (associated with <span class="math inline">\(\lambda_2\)</span>) explains the largest amount of the <em>remaining</em> variance and is uncorrelated with <span class="math inline">\(PC_1\)</span>.
- And so on.</p>
<p>The total variance in the data is preserved: <span class="math inline">\(\sum_{j=1}^{p} Var(X&#39;_j) = tr(\mathbf{S}) = \sum_{j=1}^{p} \lambda_j\)</span>. If using the correlation matrix <span class="math inline">\(\mathbf{R}\)</span>, then <span class="math inline">\(tr(\mathbf{R}) = p = \sum_{j=1}^{p} \lambda_j\)</span>.</p>
<p>The proportion of total variance explained by <span class="math inline">\(PC_j\)</span> is <span class="math inline">\(\frac{\lambda_j}{\sum_{k=1}^{p} \lambda_k}\)</span>.</p>
</div>
<div id="geometric-interpretation" class="section level2 hasAnchor" number="7.5">
<h2><span class="header-section-number">7.5</span> Geometric Interpretation<a href="principal-component-analysis.html#geometric-interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Geometrically, PCA performs a rotation of the original coordinate system (defined by the original variables) to a new coordinate system defined by the principal components.
- The origin of this new system is the mean of the data.
- The axes of this new system are the eigenvectors of the covariance/correlation matrix.
- The data, when projected onto these new axes, are the principal component scores.
- The eigenvalues represent the variance of the data along these new axes. The first PC axis is the direction in which the “data cloud” is most elongated. The second PC axis is the next most elongated direction, perpendicular to the first, and so on.</p>
</div>
<div id="steps-in-performing-pca-mathematical-summary" class="section level2 hasAnchor" number="7.6">
<h2><span class="header-section-number">7.6</span> Steps in Performing PCA (Mathematical Summary)<a href="principal-component-analysis.html#steps-in-performing-pca-mathematical-summary" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p><strong>Data Preparation</strong>: a. Organize data into an <span class="math inline">\(n \times p\)</span> matrix <span class="math inline">\(\mathbf{X}\)</span>.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li><strong>Center the data</strong>: Subtract the mean of each variable to get <span class="math inline">\(\mathbf{X}&#39;\)</span>.</li>
<li><strong>Standardize (Optional but Recommended)</strong>: If variables are on different scales, divide each centered variable by its standard deviation. If standardized, subsequent steps use the correlation matrix <span class="math inline">\(\mathbf{R}\)</span>; otherwise, use the covariance matrix <span class="math inline">\(\mathbf{S}\)</span>. Let <span class="math inline">\(\mathbf{A}\)</span> be the chosen matrix (<span class="math inline">\(\mathbf{S}\)</span> or <span class="math inline">\(\mathbf{R}\)</span>).</li>
</ol></li>
<li><p><strong>Calculate the Covariance or Correlation Matrix (<span class="math inline">\(\mathbf{A}\)</span>)</strong>:</p>
<ul>
<li>If using centered data <span class="math inline">\(\mathbf{X}&#39;\)</span>: <span class="math inline">\(\mathbf{S} = \frac{1}{n-1} (\mathbf{X}&#39;)^T \mathbf{X}&#39;\)</span>.</li>
<li>If using standardized data <span class="math inline">\(\mathbf{Z}\)</span>: <span class="math inline">\(\mathbf{R} = \frac{1}{n-1} \mathbf{Z}^T \mathbf{Z}\)</span> (or simply calculate correlations).</li>
</ul></li>
<li><p><strong>Compute Eigenvalues and Eigenvectors of <span class="math inline">\(\mathbf{A}\)</span></strong>:
Solve the eigenvalue problem <span class="math inline">\(\mathbf{A}\mathbf{v} = \lambda\mathbf{v}\)</span> to find <span class="math inline">\(p\)</span> eigenvalues <span class="math inline">\(\lambda_j\)</span> and <span class="math inline">\(p\)</span> corresponding eigenvectors <span class="math inline">\(\mathbf{v}_j\)</span>.</p></li>
<li><p><strong>Sort Eigenvalues and Eigenvectors</strong>:
Arrange the eigenvalues in descending order: <span class="math inline">\(\lambda_1 \ge \lambda_2 \ge ... \ge \lambda_p\)</span>. Sort the corresponding eigenvectors <span class="math inline">\(\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_p\)</span> accordingly. These eigenvectors are the principal component directions (weights).</p></li>
<li><p><strong>Form Principal Components (Component Scores)</strong>:
The principal component scores for each observation <span class="math inline">\(i\)</span> are calculated by projecting the (centered or standardized) data onto the principal component directions (eigenvectors). Let <span class="math inline">\(\mathbf{V}\)</span> be the <span class="math inline">\(p \times p\)</span> matrix whose columns are the eigenvectors <span class="math inline">\(\mathbf{v}_1, ..., \mathbf{v}_p\)</span>.
The matrix of principal component scores, <span class="math inline">\(\mathbf{P}\)</span> (<span class="math inline">\(n \times p\)</span>), is given by:
<span class="math display">\[\mathbf{P} = \mathbf{X}&#39;\mathbf{V}\]</span>
(If data was standardized to <span class="math inline">\(\mathbf{Z}\)</span>, then <span class="math inline">\(\mathbf{P} = \mathbf{Z}\mathbf{V}\)</span>).
The <span class="math inline">\(j\)</span>-th column of <span class="math inline">\(\mathbf{P}\)</span> contains the scores for <span class="math inline">\(PC_j\)</span>. The variance of <span class="math inline">\(PC_j\)</span> is <span class="math inline">\(\lambda_j\)</span>.</p></li>
<li><p><strong>Dimensionality Reduction: Decide How Many Components to Retain (<span class="math inline">\(k\)</span>)</strong>:
This is a crucial step. Instead of using all <span class="math inline">\(p\)</span> components, we often select the first <span class="math inline">\(k\)</span> components (<span class="math inline">\(k &lt; p\)</span>) that capture a significant amount of the total variance. Common methods include:</p>
<ul>
<li><strong>Kaiser’s Criterion</strong>: Retain PCs with eigenvalues <span class="math inline">\(\lambda_j &gt; 1\)</span> (when PCA is performed on a correlation matrix, as <span class="math inline">\(\lambda_j=1\)</span> means the PC explains as much variance as one original standardized variable).</li>
<li><strong>Scree Plot</strong>: Plot eigenvalues <span class="math inline">\(\lambda_j\)</span> against component number <span class="math inline">\(j\)</span>. Look for an “elbow” or a point where the magnitude of eigenvalues drops off sharply. Retain components before this drop.</li>
<li><strong>Proportion of Variance Explained</strong>: Retain enough PCs to explain a substantial portion of the total variance (e.g., 70-90%). The cumulative proportion of variance explained by the first <span class="math inline">\(k\)</span> components is <span class="math inline">\(\frac{\sum_{j=1}^{k} \lambda_j}{\sum_{j=1}^{p} \lambda_j}\)</span>.</li>
<li><strong>Parallel Analysis</strong>: Compares observed eigenvalues with those obtained from random datasets of the same size (<span class="math inline">\(n \times p\)</span>). Retain PCs whose eigenvalues are larger than the average (or, more stringently, the 95th percentile) eigenvalues from the random data. This helps distinguish true signals from random noise.</li>
<li><strong>Interpretability</strong>: The chosen components should be interpretable in the context of the research domain. If a component is mathematically significant but makes no theoretical sense, its value is questionable.</li>
</ul></li>
</ol>
</div>
<div id="interpreting-pca-results" class="section level2 hasAnchor" number="7.7">
<h2><span class="header-section-number">7.7</span> Interpreting PCA Results<a href="principal-component-analysis.html#interpreting-pca-results" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p><strong>Loadings (Component Loadings)</strong>:
Loadings are the correlations between the original variables and the principal components. They indicate how much an original variable “loads onto” or contributes to a particular PC. For PCA on a correlation matrix <span class="math inline">\(\mathbf{R}\)</span>, the loading of variable <span class="math inline">\(X_m\)</span> on <span class="math inline">\(PC_j\)</span> (whose direction is given by eigenvector <span class="math inline">\(\mathbf{v}_j\)</span> and variance by eigenvalue <span class="math inline">\(\lambda_j\)</span>) is:
<span class="math display">\[L_{mj} = v_{mj} \sqrt{\lambda_j}\]</span>
where <span class="math inline">\(v_{mj}\)</span> is the <span class="math inline">\(m\)</span>-th element of the <span class="math inline">\(j\)</span>-th eigenvector <span class="math inline">\(\mathbf{v}_j\)</span>.
High absolute loadings (e.g., &gt; |0.4| or |0.5|) suggest that the variable is strongly related to that PC. The pattern of loadings helps in naming or interpreting the meaning of each PC. The sum of squared loadings for a variable across all <span class="math inline">\(p\)</span> components equals its variance (1 if standardized). The sum of squared loadings for a component across all variables equals its eigenvalue.</p></li>
<li><p><strong>Component Scores</strong>:
These are the actual values of the new PC variables for each observation in your dataset (<span class="math inline">\(\mathbf{P} = \mathbf{X}&#39;\mathbf{V}\)</span>). They represent each observation’s position in the new principal component space. These scores are uncorrelated and can be used in subsequent analyses (e.g., as predictors in a regression, for clustering, or for plotting).</p></li>
</ul>
</div>
<div id="practical-example-of-pca-in-r" class="section level2 hasAnchor" number="7.8">
<h2><span class="header-section-number">7.8</span> Practical Example of PCA in R<a href="principal-component-analysis.html#practical-example-of-pca-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let's walk through an example of PCA using R. We will simulate a dataset representing responses to a hypothetical 12-item psychological questionnaire designed to measure three underlying constructs: “Social Engagement” (SE), “Emotional Stability” (ES), and “Task Focus” (TF). Each construct is measured by 4 items.</p>
<div class="sourceCode" id="cb531"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb531-1"><a href="principal-component-analysis.html#cb531-1" tabindex="-1"></a><span class="co"># Simulate data</span></span>
<span id="cb531-2"><a href="principal-component-analysis.html#cb531-2" tabindex="-1"></a><span class="co"># Number of participants</span></span>
<span id="cb531-3"><a href="principal-component-analysis.html#cb531-3" tabindex="-1"></a>n_participants <span class="ot">&lt;-</span> <span class="dv">300</span></span>
<span id="cb531-4"><a href="principal-component-analysis.html#cb531-4" tabindex="-1"></a></span>
<span id="cb531-5"><a href="principal-component-analysis.html#cb531-5" tabindex="-1"></a><span class="co"># True underlying factor structure (for simulation purposes)</span></span>
<span id="cb531-6"><a href="principal-component-analysis.html#cb531-6" tabindex="-1"></a><span class="co"># We&#39;ll simulate data as if there are three latent factors</span></span>
<span id="cb531-7"><a href="principal-component-analysis.html#cb531-7" tabindex="-1"></a><span class="co"># Factor 1: Social Engagement (Items 1-4)</span></span>
<span id="cb531-8"><a href="principal-component-analysis.html#cb531-8" tabindex="-1"></a><span class="co"># Factor 2: Emotional Stability (Items 5-8)</span></span>
<span id="cb531-9"><a href="principal-component-analysis.html#cb531-9" tabindex="-1"></a><span class="co"># Factor 3: Task Focus (Items 9-12)</span></span>
<span id="cb531-10"><a href="principal-component-analysis.html#cb531-10" tabindex="-1"></a></span>
<span id="cb531-11"><a href="principal-component-analysis.html#cb531-11" tabindex="-1"></a><span class="co"># Simulate latent factors (assuming they are somewhat correlated)</span></span>
<span id="cb531-12"><a href="principal-component-analysis.html#cb531-12" tabindex="-1"></a>factor_cor_matrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fl">1.0</span>, <span class="fl">0.3</span>, <span class="fl">0.2</span>,</span>
<span id="cb531-13"><a href="principal-component-analysis.html#cb531-13" tabindex="-1"></a>                              <span class="fl">0.3</span>, <span class="fl">1.0</span>, <span class="fl">0.4</span>,</span>
<span id="cb531-14"><a href="principal-component-analysis.html#cb531-14" tabindex="-1"></a>                              <span class="fl">0.2</span>, <span class="fl">0.4</span>, <span class="fl">1.0</span>), <span class="at">nrow =</span> <span class="dv">3</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb531-15"><a href="principal-component-analysis.html#cb531-15" tabindex="-1"></a>latent_factors <span class="ot">&lt;-</span> <span class="fu">mvrnorm</span>(n_participants, <span class="at">mu =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">3</span>), <span class="at">Sigma =</span> factor_cor_matrix)</span>
<span id="cb531-16"><a href="principal-component-analysis.html#cb531-16" tabindex="-1"></a><span class="fu">colnames</span>(latent_factors) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Latent_SE&quot;</span>, <span class="st">&quot;Latent_ES&quot;</span>, <span class="st">&quot;Latent_TF&quot;</span>)</span>
<span id="cb531-17"><a href="principal-component-analysis.html#cb531-17" tabindex="-1"></a></span>
<span id="cb531-18"><a href="principal-component-analysis.html#cb531-18" tabindex="-1"></a><span class="co"># Simulate item responses based on these factors</span></span>
<span id="cb531-19"><a href="principal-component-analysis.html#cb531-19" tabindex="-1"></a><span class="co"># Each item loads primarily on one factor, with some noise</span></span>
<span id="cb531-20"><a href="principal-component-analysis.html#cb531-20" tabindex="-1"></a>n_items <span class="ot">&lt;-</span> <span class="dv">12</span></span>
<span id="cb531-21"><a href="principal-component-analysis.html#cb531-21" tabindex="-1"></a>item_data <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">nrow =</span> n_participants, <span class="at">ncol =</span> n_items)</span>
<span id="cb531-22"><a href="principal-component-analysis.html#cb531-22" tabindex="-1"></a>item_names <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;Item&quot;</span>, <span class="dv">1</span><span class="sc">:</span>n_items)</span>
<span id="cb531-23"><a href="principal-component-analysis.html#cb531-23" tabindex="-1"></a><span class="fu">colnames</span>(item_data) <span class="ot">&lt;-</span> item_names</span>
<span id="cb531-24"><a href="principal-component-analysis.html#cb531-24" tabindex="-1"></a></span>
<span id="cb531-25"><a href="principal-component-analysis.html#cb531-25" tabindex="-1"></a><span class="co"># Define loadings (simplified for simulation) and add noise</span></span>
<span id="cb531-26"><a href="principal-component-analysis.html#cb531-26" tabindex="-1"></a><span class="co"># Items 1-4 load on SE (Latent_Factor1)</span></span>
<span id="cb531-27"><a href="principal-component-analysis.html#cb531-27" tabindex="-1"></a>item_data[, <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>] <span class="ot">&lt;-</span> latent_factors[, <span class="dv">1</span>] <span class="sc">*</span> <span class="fl">0.7</span> <span class="sc">+</span> <span class="fu">rnorm</span>(n_participants <span class="sc">*</span> <span class="dv">4</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fl">0.7</span><span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb531-28"><a href="principal-component-analysis.html#cb531-28" tabindex="-1"></a><span class="co"># Items 5-8 load on ES (Latent_Factor2)</span></span>
<span id="cb531-29"><a href="principal-component-analysis.html#cb531-29" tabindex="-1"></a>item_data[, <span class="dv">5</span><span class="sc">:</span><span class="dv">8</span>] <span class="ot">&lt;-</span> latent_factors[, <span class="dv">2</span>] <span class="sc">*</span> <span class="fl">0.7</span> <span class="sc">+</span> <span class="fu">rnorm</span>(n_participants <span class="sc">*</span> <span class="dv">4</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fl">0.7</span><span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb531-30"><a href="principal-component-analysis.html#cb531-30" tabindex="-1"></a><span class="co"># Items 9-12 load on TF (Latent_Factor3)</span></span>
<span id="cb531-31"><a href="principal-component-analysis.html#cb531-31" tabindex="-1"></a>item_data[, <span class="dv">9</span><span class="sc">:</span><span class="dv">12</span>] <span class="ot">&lt;-</span> latent_factors[, <span class="dv">3</span>] <span class="sc">*</span> <span class="fl">0.7</span> <span class="sc">+</span> <span class="fu">rnorm</span>(n_participants <span class="sc">*</span> <span class="dv">4</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fu">sqrt</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fl">0.7</span><span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb531-32"><a href="principal-component-analysis.html#cb531-32" tabindex="-1"></a></span>
<span id="cb531-33"><a href="principal-component-analysis.html#cb531-33" tabindex="-1"></a><span class="co"># Convert to a data frame</span></span>
<span id="cb531-34"><a href="principal-component-analysis.html#cb531-34" tabindex="-1"></a>psych_data <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(item_data)</span>
<span id="cb531-35"><a href="principal-component-analysis.html#cb531-35" tabindex="-1"></a></span>
<span id="cb531-36"><a href="principal-component-analysis.html#cb531-36" tabindex="-1"></a><span class="co"># Display first few rows of the data</span></span>
<span id="cb531-37"><a href="principal-component-analysis.html#cb531-37" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;First 6 rows of the simulated psychological data:</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>First 6 rows of the simulated psychological data:</code></pre>
<div class="sourceCode" id="cb533"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb533-1"><a href="principal-component-analysis.html#cb533-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">head</span>(psych_data))</span></code></pre></div>
<pre><code>       Item1       Item2      Item3      Item4      Item5      Item6      Item7
1 -1.2011397 -0.03425583 -1.0632191 -1.3977075 -0.8226692 -1.0247631 -2.1123919
2 -1.0632315 -1.03909084 -0.7175459 -0.9655759  0.2319257 -0.4244909 -0.1920228
3  0.4214602  0.81561782 -0.4367193  0.1665700  1.5499959  0.5633339 -0.1946373
4  0.4488172 -1.25582882 -0.2738833  0.1757975  0.6548033  0.4611393  0.7442183
5  0.7131293  0.38863904  0.7386513  1.0722268  0.1216722  0.3026221 -0.8429254
6  0.4672031  1.69640652  2.4323678  1.1413889  0.2005913  1.2017979  1.0517729
        Item8       Item9     Item10       Item11     Item12
1 -0.78707668  0.08263518  0.1471474 -0.996255250  0.7646070
2 -0.27909873  1.69026955  0.9859125  0.006588846 -0.5088199
3 -0.05117208  1.74125104  1.2857078  1.801293655  2.0435489
4  0.25718954 -0.25141413 -0.4474001  0.414305389 -1.5695793
5  1.65185543 -0.88845171  0.9211174  0.020053295 -0.3235745
6  0.99197434  0.50337566  1.3917258  1.356228776  0.4664513</code></pre>
<div class="sourceCode" id="cb535"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb535-1"><a href="principal-component-analysis.html#cb535-1" tabindex="-1"></a><span class="co"># Check correlations among items</span></span>
<span id="cb535-2"><a href="principal-component-analysis.html#cb535-2" tabindex="-1"></a>cor_matrix <span class="ot">&lt;-</span> <span class="fu">cor</span>(psych_data)</span>
<span id="cb535-3"><a href="principal-component-analysis.html#cb535-3" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Correlation matrix of items (first 6x6 shown):</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>
Correlation matrix of items (first 6x6 shown):</code></pre>
<div class="sourceCode" id="cb537"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb537-1"><a href="principal-component-analysis.html#cb537-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(cor_matrix[<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>], <span class="dv">2</span>))</span></code></pre></div>
<pre><code>      Item1 Item2 Item3 Item4 Item5 Item6
Item1  1.00  0.51  0.51  0.46  0.13  0.09
Item2  0.51  1.00  0.39  0.45  0.03  0.10
Item3  0.51  0.39  1.00  0.39  0.04  0.08
Item4  0.46  0.45  0.39  1.00  0.12  0.19
Item5  0.13  0.03  0.04  0.12  1.00  0.51
Item6  0.09  0.10  0.08  0.19  0.51  1.00</code></pre>
<div class="sourceCode" id="cb539"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb539-1"><a href="principal-component-analysis.html#cb539-1" tabindex="-1"></a><span class="co"># Visualize the full correlation matrix</span></span>
<span id="cb539-2"><a href="principal-component-analysis.html#cb539-2" tabindex="-1"></a><span class="co"># Using a cool color palette as per style guide</span></span>
<span id="cb539-3"><a href="principal-component-analysis.html#cb539-3" tabindex="-1"></a><span class="co"># The style guide asks for cool colors.</span></span>
<span id="cb539-4"><a href="principal-component-analysis.html#cb539-4" tabindex="-1"></a>cool_palette_corr <span class="ot">&lt;-</span> <span class="fu">colorRampPalette</span>(<span class="fu">c</span>(<span class="st">&quot;#F0F8FF&quot;</span>, <span class="st">&quot;#A6D8F0&quot;</span>, <span class="st">&quot;#4682B4&quot;</span>, <span class="st">&quot;#003366&quot;</span>))(<span class="dv">200</span>) <span class="co"># AliceBlue to dark blue</span></span>
<span id="cb539-5"><a href="principal-component-analysis.html#cb539-5" tabindex="-1"></a></span>
<span id="cb539-6"><a href="principal-component-analysis.html#cb539-6" tabindex="-1"></a><span class="fu">corrplot</span>(cor_matrix, <span class="at">method =</span> <span class="st">&quot;color&quot;</span>, <span class="at">order =</span> <span class="st">&quot;hclust&quot;</span>, <span class="co"># Hierarchical clustering to group similar items</span></span>
<span id="cb539-7"><a href="principal-component-analysis.html#cb539-7" tabindex="-1"></a>         <span class="at">col =</span> cool_palette_corr, </span>
<span id="cb539-8"><a href="principal-component-analysis.html#cb539-8" tabindex="-1"></a>         <span class="at">tl.col =</span> <span class="st">&quot;grey20&quot;</span>, <span class="at">tl.srt =</span> <span class="dv">45</span>, <span class="co"># Text label color and rotation</span></span>
<span id="cb539-9"><a href="principal-component-analysis.html#cb539-9" tabindex="-1"></a>         <span class="at">addCoef.col =</span> <span class="st">&quot;black&quot;</span>, <span class="at">number.cex =</span> <span class="fl">0.4</span>, <span class="co"># Add correlation coefficients</span></span>
<span id="cb539-10"><a href="principal-component-analysis.html#cb539-10" tabindex="-1"></a>         <span class="at">cl.ratio =</span> <span class="fl">0.2</span>, <span class="co"># Width of the color legend</span></span>
<span id="cb539-11"><a href="principal-component-analysis.html#cb539-11" tabindex="-1"></a>         <span class="at">type =</span> <span class="st">&quot;upper&quot;</span>, <span class="co"># Show only upper triangle</span></span>
<span id="cb539-12"><a href="principal-component-analysis.html#cb539-12" tabindex="-1"></a>         <span class="at">diag =</span> <span class="cn">FALSE</span>, <span class="co"># Don&#39;t show diagonal</span></span>
<span id="cb539-13"><a href="principal-component-analysis.html#cb539-13" tabindex="-1"></a>         <span class="at">title =</span> <span class="st">&quot;Item Correlation Matrix&quot;</span>, <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">1.5</span>,<span class="dv">0</span>)) <span class="co"># Adjust margins for title</span></span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:pca-r-example-data-simulation"></span>
<img src="psychological_measurement_handbook_files/figure-html/pca-r-example-data-simulation-1.png" alt="Correlation matrix of the simulated psychological questionnaire items. Darker colors indicate stronger positive correlations. The clustering of items (1-4, 5-8, 9-12) suggests underlying constructs." width="672" />
<p class="caption">
Figure 7.1: Correlation matrix of the simulated psychological questionnaire items. Darker colors indicate stronger positive correlations. The clustering of items (1-4, 5-8, 9-12) suggests underlying constructs.
</p>
</div>
<div class="sourceCode" id="cb540"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb540-1"><a href="principal-component-analysis.html#cb540-1" tabindex="-1"></a><span class="co"># Note: corrplot has its own theming. Border requirements from style_guide.md apply mainly to ggplot.</span></span></code></pre></div>
<p><strong>Explanation of Data Simulation:</strong>
We simulated data for 300 participants on 12 questionnaire items. These items were intentionally designed to reflect three underlying psychological constructs: Social Engagement (Items 1-4), Emotional Stability (Items 5-8), and Task Focus (Items 9-12). Each item’s score is primarily determined by its respective latent factor (e.g., Item1 by Latent_SE), with an additional component of random noise to make the data more realistic. The strength of the relationship between an item and its factor was set (loading of 0.7), and the remaining variance is noise. The latent factors themselves were simulated to be moderately correlated. The correlation plot generated by <code>corrplot</code> should visually suggest these groupings of items through blocks of stronger positive correlations.</p>
</div>
<div id="performing-pca" class="section level2 hasAnchor" number="7.9">
<h2><span class="header-section-number">7.9</span> Performing PCA<a href="principal-component-analysis.html#performing-pca" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now, let's perform PCA on this dataset. Since the items are simulated to be on a similar scale (due to the way noise was added relative to factor influence), standardizing them (which <code>prcomp</code> does by default with <code>scale. = TRUE</code>) is still good practice and ensures PCA is based on the correlation matrix. This is generally recommended unless variables are truly commensurable.</p>
<div class="sourceCode" id="cb541"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb541-1"><a href="principal-component-analysis.html#cb541-1" tabindex="-1"></a><span class="co"># Perform PCA using prcomp()</span></span>
<span id="cb541-2"><a href="principal-component-analysis.html#cb541-2" tabindex="-1"></a><span class="co"># center = TRUE: centers variables to have mean 0.</span></span>
<span id="cb541-3"><a href="principal-component-analysis.html#cb541-3" tabindex="-1"></a><span class="co"># scale. = TRUE: scales variables to have unit variance (standardizes them).</span></span>
<span id="cb541-4"><a href="principal-component-analysis.html#cb541-4" tabindex="-1"></a><span class="co"># These are defaults for prcomp if data is passed directly, but explicit is good.</span></span>
<span id="cb541-5"><a href="principal-component-analysis.html#cb541-5" tabindex="-1"></a>pca_results <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(psych_data, <span class="at">center =</span> <span class="cn">TRUE</span>, <span class="at">scale. =</span> <span class="cn">TRUE</span>)</span>
<span id="cb541-6"><a href="principal-component-analysis.html#cb541-6" tabindex="-1"></a></span>
<span id="cb541-7"><a href="principal-component-analysis.html#cb541-7" tabindex="-1"></a><span class="co"># Summary of PCA results</span></span>
<span id="cb541-8"><a href="principal-component-analysis.html#cb541-8" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Summary of PCA results (Standard Deviations and Proportions of Variance):</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>
Summary of PCA results (Standard Deviations and Proportions of Variance):</code></pre>
<div class="sourceCode" id="cb543"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb543-1"><a href="principal-component-analysis.html#cb543-1" tabindex="-1"></a><span class="co"># The summary() output for prcomp shows standard deviations, proportion of variance, and cumulative proportion.</span></span>
<span id="cb543-2"><a href="principal-component-analysis.html#cb543-2" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">summary</span>(pca_results))</span></code></pre></div>
<pre><code>Importance of components:
                          PC1    PC2    PC3     PC4     PC5     PC6     PC7
Standard deviation     1.8190 1.5027 1.3462 0.82751 0.79987 0.76226 0.71602
Proportion of Variance 0.2757 0.1882 0.1510 0.05706 0.05332 0.04842 0.04272
Cumulative Proportion  0.2757 0.4639 0.6149 0.67198 0.72529 0.77371 0.81644
                           PC8    PC9    PC10   PC11   PC12
Standard deviation     0.70052 0.6894 0.65974 0.6481 0.6177
Proportion of Variance 0.04089 0.0396 0.03627 0.0350 0.0318
Cumulative Proportion  0.85733 0.8969 0.93321 0.9682 1.0000</code></pre>
<div class="sourceCode" id="cb545"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb545-1"><a href="principal-component-analysis.html#cb545-1" tabindex="-1"></a><span class="co"># Eigenvalues (which are the variances of the Principal Components)</span></span>
<span id="cb545-2"><a href="principal-component-analysis.html#cb545-2" tabindex="-1"></a><span class="co"># pca_results$sdev contains the standard deviations of the PCs. We square them to get eigenvalues.</span></span>
<span id="cb545-3"><a href="principal-component-analysis.html#cb545-3" tabindex="-1"></a>eigenvalues <span class="ot">&lt;-</span> pca_results<span class="sc">$</span>sdev<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb545-4"><a href="principal-component-analysis.html#cb545-4" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Eigenvalues (Variances of Principal Components):</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>
Eigenvalues (Variances of Principal Components):</code></pre>
<div class="sourceCode" id="cb547"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb547-1"><a href="principal-component-analysis.html#cb547-1" tabindex="-1"></a><span class="fu">print</span>(eigenvalues)</span></code></pre></div>
<pre><code> [1] 3.3086466 2.2580694 1.8122347 0.6847697 0.6397933 0.5810449 0.5126813
 [8] 0.4907352 0.4752388 0.4352540 0.4199782 0.3815540</code></pre>
<div class="sourceCode" id="cb549"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb549-1"><a href="principal-component-analysis.html#cb549-1" tabindex="-1"></a><span class="co"># Proportion of Variance Explained by each PC</span></span>
<span id="cb549-2"><a href="principal-component-analysis.html#cb549-2" tabindex="-1"></a><span class="co"># This is also in summary(pca_results), but can be calculated directly</span></span>
<span id="cb549-3"><a href="principal-component-analysis.html#cb549-3" tabindex="-1"></a>prop_variance_explained <span class="ot">&lt;-</span> eigenvalues <span class="sc">/</span> <span class="fu">sum</span>(eigenvalues)</span>
<span id="cb549-4"><a href="principal-component-analysis.html#cb549-4" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Proportion of Variance Explained by each PC:</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>
Proportion of Variance Explained by each PC:</code></pre>
<div class="sourceCode" id="cb551"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb551-1"><a href="principal-component-analysis.html#cb551-1" tabindex="-1"></a><span class="fu">print</span>(prop_variance_explained)</span></code></pre></div>
<pre><code> [1] 0.27572055 0.18817245 0.15101956 0.05706414 0.05331611 0.04842041 0.04272344
 [8] 0.04089460 0.03960323 0.03627116 0.03499818 0.03179617</code></pre>
<div class="sourceCode" id="cb553"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb553-1"><a href="principal-component-analysis.html#cb553-1" tabindex="-1"></a><span class="co"># Cumulative Proportion of Variance Explained</span></span>
<span id="cb553-2"><a href="principal-component-analysis.html#cb553-2" tabindex="-1"></a>cumulative_variance_explained <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(prop_variance_explained)</span>
<span id="cb553-3"><a href="principal-component-analysis.html#cb553-3" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Cumulative Proportion of Variance Explained:</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>
Cumulative Proportion of Variance Explained:</code></pre>
<div class="sourceCode" id="cb555"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb555-1"><a href="principal-component-analysis.html#cb555-1" tabindex="-1"></a><span class="fu">print</span>(cumulative_variance_explained)</span></code></pre></div>
<pre><code> [1] 0.2757206 0.4638930 0.6149126 0.6719767 0.7252928 0.7737132 0.8164367
 [8] 0.8573313 0.8969345 0.9332056 0.9682038 1.0000000</code></pre>
<p><strong>Explanation of PCA Output:</strong>
- The <code>summary(pca_results)</code> output is key. It provides:
- <strong>Standard deviation</strong>: The standard deviation of each principal component (<code>pca_results$sdev</code>).
- <strong>Proportion of Variance</strong>: The proportion of the total variance in the dataset that each principal component accounts for. This is calculated as (Eigenvalue of PC / Sum of all Eigenvalues).
- <strong>Cumulative Proportion</strong>: The cumulative sum of the proportion of variance explained by the components, in order. This tells us how much variance is captured by retaining the first <span class="math inline">\(k\)</span> components.
- <code>eigenvalues &lt;- pca_results$sdev^2</code>: We explicitly calculate the eigenvalues by squaring the standard deviations of the PCs. Each eigenvalue represents the variance of its corresponding PC.
- The output shows that the first few components explain a substantial amount of variance, and this amount decreases for subsequent components. Our goal is to retain enough components to capture most of the meaningful variation without overfitting to noise.</p>
</div>
<div id="scree-plot-deciding-on-the-number-of-components" class="section level2 hasAnchor" number="7.10">
<h2><span class="header-section-number">7.10</span> Scree Plot: Deciding on the Number of Components<a href="principal-component-analysis.html#scree-plot-deciding-on-the-number-of-components" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A scree plot is a graphical tool that helps visualize the eigenvalues in descending order, aiding in the decision of how many principal components to retain for further analysis.</p>
<div class="sourceCode" id="cb557"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb557-1"><a href="principal-component-analysis.html#cb557-1" tabindex="-1"></a><span class="co"># Create a data frame for the scree plot</span></span>
<span id="cb557-2"><a href="principal-component-analysis.html#cb557-2" tabindex="-1"></a>scree_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb557-3"><a href="principal-component-analysis.html#cb557-3" tabindex="-1"></a>  <span class="at">Component =</span> <span class="fu">factor</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(eigenvalues)), <span class="co"># Factor for discrete axis</span></span>
<span id="cb557-4"><a href="principal-component-analysis.html#cb557-4" tabindex="-1"></a>  <span class="at">Eigenvalue =</span> eigenvalues</span>
<span id="cb557-5"><a href="principal-component-analysis.html#cb557-5" tabindex="-1"></a>)</span>
<span id="cb557-6"><a href="principal-component-analysis.html#cb557-6" tabindex="-1"></a></span>
<span id="cb557-7"><a href="principal-component-analysis.html#cb557-7" tabindex="-1"></a><span class="co"># Scree Plot using ggplot2, styled with theme_psych_book</span></span>
<span id="cb557-8"><a href="principal-component-analysis.html#cb557-8" tabindex="-1"></a>scree_plot_gg <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(scree_data, <span class="fu">aes</span>(<span class="at">x =</span> Component, <span class="at">y =</span> Eigenvalue, <span class="at">group =</span> <span class="dv">1</span>)) <span class="sc">+</span> <span class="co"># group=1 for line</span></span>
<span id="cb557-9"><a href="principal-component-analysis.html#cb557-9" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">color =</span> cool_colors[<span class="dv">1</span>], <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb557-10"><a href="principal-component-analysis.html#cb557-10" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">color =</span> cool_colors[<span class="dv">1</span>], <span class="at">size =</span> <span class="dv">3</span>, <span class="at">shape =</span> <span class="dv">16</span>) <span class="sc">+</span></span>
<span id="cb557-11"><a href="principal-component-analysis.html#cb557-11" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">color =</span> cool_colors[<span class="dv">5</span>], <span class="at">linewidth =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb557-12"><a href="principal-component-analysis.html#cb557-12" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="at">x =</span> <span class="fu">length</span>(eigenvalues) <span class="sc">*</span> <span class="fl">0.7</span>, <span class="at">y =</span> <span class="fl">1.15</span>, </span>
<span id="cb557-13"><a href="principal-component-analysis.html#cb557-13" tabindex="-1"></a>           <span class="at">label =</span> <span class="st">&quot;Kaiser&#39;s Criterion (Eigenvalue = 1)&quot;</span>, </span>
<span id="cb557-14"><a href="principal-component-analysis.html#cb557-14" tabindex="-1"></a>           <span class="at">color =</span> cool_colors[<span class="dv">5</span>], <span class="at">size =</span> <span class="fl">3.5</span>, <span class="at">hjust =</span> <span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb557-15"><a href="principal-component-analysis.html#cb557-15" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb557-16"><a href="principal-component-analysis.html#cb557-16" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Scree Plot of Eigenvalues&quot;</span>,</span>
<span id="cb557-17"><a href="principal-component-analysis.html#cb557-17" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Principal Component Number&quot;</span>,</span>
<span id="cb557-18"><a href="principal-component-analysis.html#cb557-18" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Eigenvalue (Variance Explained)&quot;</span></span>
<span id="cb557-19"><a href="principal-component-analysis.html#cb557-19" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb557-20"><a href="principal-component-analysis.html#cb557-20" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="cn">NA</span>), <span class="at">expand =</span> <span class="fu">expansion</span>(<span class="at">mult =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.05</span>))) <span class="sc">+</span> <span class="co"># Ensure y-axis starts at 0</span></span>
<span id="cb557-21"><a href="principal-component-analysis.html#cb557-21" tabindex="-1"></a>  <span class="fu">scale_x_discrete</span>(<span class="at">labels =</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(eigenvalues)) <span class="sc">+</span> <span class="co"># Ensure all component numbers are shown</span></span>
<span id="cb557-22"><a href="principal-component-analysis.html#cb557-22" tabindex="-1"></a>  <span class="fu">theme_psych_book</span>() <span class="co"># Apply custom theme</span></span>
<span id="cb557-23"><a href="principal-component-analysis.html#cb557-23" tabindex="-1"></a></span>
<span id="cb557-24"><a href="principal-component-analysis.html#cb557-24" tabindex="-1"></a><span class="fu">print</span>(scree_plot_gg)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:pca-r-scree-plot"></span>
<img src="psychological_measurement_handbook_files/figure-html/pca-r-scree-plot-1.png" alt="Scree plot of eigenvalues. The 'elbow' and Kaiser's criterion (eigenvalue &gt; 1) suggest the number of components to retain. Parallel analysis results are also considered." width="672" />
<p class="caption">
Figure 7.2: Scree plot of eigenvalues. The ‘elbow’ and Kaiser’s criterion (eigenvalue &gt; 1) suggest the number of components to retain. Parallel analysis results are also considered.
</p>
</div>
<div class="sourceCode" id="cb558"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb558-1"><a href="principal-component-analysis.html#cb558-1" tabindex="-1"></a><span class="co"># Parallel Analysis (another method to determine number of components)</span></span>
<span id="cb558-2"><a href="principal-component-analysis.html#cb558-2" tabindex="-1"></a><span class="co"># This requires the &#39;psych&#39; package. It compares observed eigenvalues to those from random data.</span></span>
<span id="cb558-3"><a href="principal-component-analysis.html#cb558-3" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Performing Parallel Analysis (from psych package):</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>
Performing Parallel Analysis (from psych package):</code></pre>
<div class="sourceCode" id="cb560"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb560-1"><a href="principal-component-analysis.html#cb560-1" tabindex="-1"></a><span class="co"># fa.parallel plots its own graph and outputs text suggestions.</span></span>
<span id="cb560-2"><a href="principal-component-analysis.html#cb560-2" tabindex="-1"></a><span class="co"># We set fa=&quot;pc&quot; for Principal Components Analysis. n.iter for number of random datasets.</span></span>
<span id="cb560-3"><a href="principal-component-analysis.html#cb560-3" tabindex="-1"></a><span class="co"># The plot generated by fa.parallel is separate from our ggplot scree plot.</span></span>
<span id="cb560-4"><a href="principal-component-analysis.html#cb560-4" tabindex="-1"></a><span class="co"># We capture the console output to report its suggestion.</span></span>
<span id="cb560-5"><a href="principal-component-analysis.html#cb560-5" tabindex="-1"></a><span class="co"># Suppress the automatic plot from fa.parallel for cleaner Rmd output, focus on its numeric suggestion.</span></span>
<span id="cb560-6"><a href="principal-component-analysis.html#cb560-6" tabindex="-1"></a><span class="co"># The plot from fa.parallel is useful but we already have a styled scree plot.</span></span>
<span id="cb560-7"><a href="principal-component-analysis.html#cb560-7" tabindex="-1"></a><span class="co"># We are primarily interested in its numerical suggestion for ncomp.</span></span>
<span id="cb560-8"><a href="principal-component-analysis.html#cb560-8" tabindex="-1"></a><span class="co"># To avoid issues with graphical devices in some environments, we can run it but not display its plot directly here.</span></span>
<span id="cb560-9"><a href="principal-component-analysis.html#cb560-9" tabindex="-1"></a><span class="co"># The console output will still give the suggestion.</span></span>
<span id="cb560-10"><a href="principal-component-analysis.html#cb560-10" tabindex="-1"></a>parallel_analysis_results <span class="ot">&lt;-</span> psych<span class="sc">::</span><span class="fu">fa.parallel</span>(psych_data, <span class="at">fa =</span> <span class="st">&quot;pc&quot;</span>, <span class="at">n.iter =</span> <span class="dv">100</span>, </span>
<span id="cb560-11"><a href="principal-component-analysis.html#cb560-11" tabindex="-1"></a>                                                <span class="at">show.legend =</span> <span class="cn">FALSE</span>, <span class="at">main =</span> <span class="st">&quot;Parallel Analysis Scree Plot (psych package)&quot;</span>,</span>
<span id="cb560-12"><a href="principal-component-analysis.html#cb560-12" tabindex="-1"></a>                                                <span class="at">plot =</span> <span class="cn">FALSE</span>) <span class="co"># Set plot=FALSE to suppress its direct plot</span></span></code></pre></div>
<pre><code>Parallel analysis suggests that the number of factors =  NA  and the number of components =  3 </code></pre>
<div class="sourceCode" id="cb562"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb562-1"><a href="principal-component-analysis.html#cb562-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">paste</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Parallel analysis (psych::fa.parallel) suggests retaining&quot;</span>, parallel_analysis_results<span class="sc">$</span>ncomp, <span class="st">&quot;components based on comparison with random data eigenvalues.</span><span class="sc">\n</span><span class="st">&quot;</span>))</span></code></pre></div>
<pre><code>
Parallel analysis (psych::fa.parallel) suggests retaining 3 components based on comparison with random data eigenvalues.</code></pre>
<div class="sourceCode" id="cb564"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb564-1"><a href="principal-component-analysis.html#cb564-1" tabindex="-1"></a><span class="co"># The object parallel_analysis_results also contains $pc.values (observed eigenvalues) </span></span>
<span id="cb564-2"><a href="principal-component-analysis.html#cb564-2" tabindex="-1"></a><span class="co"># and $pc.simr (simulated random eigenvalues) which could be plotted manually if desired.</span></span></code></pre></div>
<p><strong>Explanation of Scree Plot and Component Selection:</strong>
The scree plot is a critical diagnostic tool.
- <strong>Visual Inspection</strong>: The plot shows eigenvalues on the y-axis and component number on the x-axis. We look for an “elbow” – a point where the steep descent of eigenvalues sharply flattens. Components to the left of (and including) the elbow are typically retained. In our simulated data, we expect a clear elbow after the 3rd component, as we designed the data with three underlying factors.
- <strong>Kaiser's Criterion</strong>: The horizontal dashed line at Eigenvalue = 1 represents Kaiser's criterion. This rule suggests retaining components with eigenvalues greater than 1. This is because, for standardized data, an eigenvalue of 1 means the component explains as much variance as a single original variable. Our plot should show approximately 3 components above this line.
- <strong>Parallel Analysis</strong>: The <code>psych::fa.parallel</code> function performs a more statistically grounded test. It generates random datasets with the same number of variables and observations as ours, performs PCA on them, and calculates their eigenvalues. It then compares our actual data's eigenvalues to the distribution (typically mean or 95th percentile) of eigenvalues from these random datasets. We retain components whose eigenvalues are significantly larger than those expected from random noise. The output <code>parallel_analysis_results$ncomp</code> gives this suggested number. For our data, this should also point to 3 components.</p>
<p>If these methods (elbow, Kaiser's criterion, Parallel Analysis) converge on a similar number of components, we can be more confident in our choice. Here, we anticipate all signs will point to retaining 3 components.</p>
</div>
<div id="loadings-interpreting-the-components" class="section level2 hasAnchor" number="7.11">
<h2><span class="header-section-number">7.11</span> Loadings: Interpreting the Components<a href="principal-component-analysis.html#loadings-interpreting-the-components" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Loadings are the correlations between the original items and the retained principal components. They are crucial for understanding the psychological meaning of each component.</p>
<div class="sourceCode" id="cb565"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb565-1"><a href="principal-component-analysis.html#cb565-1" tabindex="-1"></a><span class="co"># We decided to retain 3 components based on scree plot and parallel analysis</span></span>
<span id="cb565-2"><a href="principal-component-analysis.html#cb565-2" tabindex="-1"></a>num_components_to_retain <span class="ot">&lt;-</span> <span class="dv">3</span> <span class="co"># This should match the suggestion from parallel analysis and scree plot</span></span>
<span id="cb565-3"><a href="principal-component-analysis.html#cb565-3" tabindex="-1"></a></span>
<span id="cb565-4"><a href="principal-component-analysis.html#cb565-4" tabindex="-1"></a><span class="co"># Extract loadings. In prcomp, pca_results$rotation contains the eigenvectors (weights).</span></span>
<span id="cb565-5"><a href="principal-component-analysis.html#cb565-5" tabindex="-1"></a><span class="co"># To get loadings (correlations between variables and PCs for standardized data),</span></span>
<span id="cb565-6"><a href="principal-component-analysis.html#cb565-6" tabindex="-1"></a><span class="co"># we multiply eigenvectors by the square root of eigenvalues (standard deviations of PCs).</span></span>
<span id="cb565-7"><a href="principal-component-analysis.html#cb565-7" tabindex="-1"></a><span class="co"># Loadings = Eigenvectors * sqrt(Eigenvalues)</span></span>
<span id="cb565-8"><a href="principal-component-analysis.html#cb565-8" tabindex="-1"></a><span class="co"># Eigenvectors are in pca_results$rotation</span></span>
<span id="cb565-9"><a href="principal-component-analysis.html#cb565-9" tabindex="-1"></a><span class="co"># Eigenvalues are pca_results$sdev^2</span></span>
<span id="cb565-10"><a href="principal-component-analysis.html#cb565-10" tabindex="-1"></a>loadings_matrix_calc <span class="ot">&lt;-</span> pca_results<span class="sc">$</span>rotation[, <span class="dv">1</span><span class="sc">:</span>num_components_to_retain] <span class="sc">%*%</span> <span class="fu">diag</span>(pca_results<span class="sc">$</span>sdev[<span class="dv">1</span><span class="sc">:</span>num_components_to_retain])</span>
<span id="cb565-11"><a href="principal-component-analysis.html#cb565-11" tabindex="-1"></a><span class="fu">rownames</span>(loadings_matrix_calc) <span class="ot">&lt;-</span> item_names</span>
<span id="cb565-12"><a href="principal-component-analysis.html#cb565-12" tabindex="-1"></a><span class="fu">colnames</span>(loadings_matrix_calc) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;PC&quot;</span>, <span class="dv">1</span><span class="sc">:</span>num_components_to_retain)</span>
<span id="cb565-13"><a href="principal-component-analysis.html#cb565-13" tabindex="-1"></a></span>
<span id="cb565-14"><a href="principal-component-analysis.html#cb565-14" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Loadings Matrix (Correlations between items and PCs for first&quot;</span>, num_components_to_retain, <span class="st">&quot;PCs):</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>
Loadings Matrix (Correlations between items and PCs for first 3 PCs):</code></pre>
<div class="sourceCode" id="cb567"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb567-1"><a href="principal-component-analysis.html#cb567-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">round</span>(loadings_matrix_calc, <span class="dv">3</span>))</span></code></pre></div>
<pre><code>         PC1    PC2    PC3
Item1  0.341  0.707  0.233
Item2  0.319  0.662  0.222
Item3  0.314  0.638  0.222
Item4  0.397  0.628  0.092
Item5  0.630 -0.073 -0.472
Item6  0.640 -0.017 -0.462
Item7  0.608 -0.064 -0.471
Item8  0.593 -0.002 -0.539
Item9  0.597 -0.297  0.448
Item10 0.598 -0.376  0.310
Item11 0.580 -0.348  0.469
Item12 0.506 -0.398  0.431</code></pre>
<div class="sourceCode" id="cb569"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb569-1"><a href="principal-component-analysis.html#cb569-1" tabindex="-1"></a><span class="co"># For better interpretation, we often look for loadings &gt; |0.4| or so.</span></span>
<span id="cb569-2"><a href="principal-component-analysis.html#cb569-2" tabindex="-1"></a><span class="co"># Let\&#39;s create a plot of these loadings using ggplot2.</span></span>
<span id="cb569-3"><a href="principal-component-analysis.html#cb569-3" tabindex="-1"></a>loadings_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(loadings_matrix_calc)</span>
<span id="cb569-4"><a href="principal-component-analysis.html#cb569-4" tabindex="-1"></a>loadings_df<span class="sc">$</span>Item <span class="ot">&lt;-</span> <span class="fu">rownames</span>(loadings_df)</span>
<span id="cb569-5"><a href="principal-component-analysis.html#cb569-5" tabindex="-1"></a></span>
<span id="cb569-6"><a href="principal-component-analysis.html#cb569-6" tabindex="-1"></a>loadings_long_df <span class="ot">&lt;-</span> loadings_df <span class="sc">%&gt;%</span></span>
<span id="cb569-7"><a href="principal-component-analysis.html#cb569-7" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">starts_with</span>(<span class="st">&quot;PC&quot;</span>), <span class="at">names_to =</span> <span class="st">&quot;Component&quot;</span>, <span class="at">values_to =</span> <span class="st">&quot;Loading&quot;</span>)</span>
<span id="cb569-8"><a href="principal-component-analysis.html#cb569-8" tabindex="-1"></a></span>
<span id="cb569-9"><a href="principal-component-analysis.html#cb569-9" tabindex="-1"></a><span class="co"># Ensure Component is ordered for plotting (PC1, PC2, PC3)</span></span>
<span id="cb569-10"><a href="principal-component-analysis.html#cb569-10" tabindex="-1"></a>loadings_long_df<span class="sc">$</span>Component <span class="ot">&lt;-</span> <span class="fu">factor</span>(loadings_long_df<span class="sc">$</span>Component, <span class="at">levels =</span> <span class="fu">paste0</span>(<span class="st">&quot;PC&quot;</span>, <span class="dv">1</span><span class="sc">:</span>num_components_to_retain))</span>
<span id="cb569-11"><a href="principal-component-analysis.html#cb569-11" tabindex="-1"></a><span class="co"># Ensure Item order is maintained for clarity in the plot</span></span>
<span id="cb569-12"><a href="principal-component-analysis.html#cb569-12" tabindex="-1"></a>loadings_long_df<span class="sc">$</span>Item <span class="ot">&lt;-</span> <span class="fu">factor</span>(loadings_long_df<span class="sc">$</span>Item, <span class="at">levels =</span> item_names) </span>
<span id="cb569-13"><a href="principal-component-analysis.html#cb569-13" tabindex="-1"></a></span>
<span id="cb569-14"><a href="principal-component-analysis.html#cb569-14" tabindex="-1"></a>loadings_plot_gg <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(loadings_long_df, <span class="fu">aes</span>(<span class="at">x =</span> Item, <span class="at">y =</span> Loading, <span class="at">fill =</span> Component)) <span class="sc">+</span></span>
<span id="cb569-15"><a href="principal-component-analysis.html#cb569-15" tabindex="-1"></a>  <span class="fu">geom_col</span>(<span class="at">position =</span> <span class="st">&quot;dodge&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">+</span> <span class="co"># geom_col is equivalent to geom_bar(stat=&quot;identity&quot;)</span></span>
<span id="cb569-16"><a href="principal-component-analysis.html#cb569-16" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> Component, <span class="at">ncol =</span> <span class="dv">1</span>, <span class="at">scales =</span> <span class="st">&quot;free_x&quot;</span>) <span class="sc">+</span> <span class="co"># Separate plot for each component\&#39;s loadings</span></span>
<span id="cb569-17"><a href="principal-component-analysis.html#cb569-17" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">get_palette</span>(num_components_to_retain, <span class="at">type =</span> <span class="st">&quot;sequential&quot;</span>)) <span class="sc">+</span> <span class="co"># Use theme\&#39;s palette</span></span>
<span id="cb569-18"><a href="principal-component-analysis.html#cb569-18" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>, <span class="at">color =</span> <span class="st">&quot;grey50&quot;</span>, <span class="at">linewidth =</span> <span class="fl">0.5</span>) <span class="sc">+</span> <span class="co"># Line at y=0</span></span>
<span id="cb569-19"><a href="principal-component-analysis.html#cb569-19" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb569-20"><a href="principal-component-analysis.html#cb569-20" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Item Loadings on Principal Components&quot;</span>,</span>
<span id="cb569-21"><a href="principal-component-analysis.html#cb569-21" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Questionnaire Item&quot;</span>,</span>
<span id="cb569-22"><a href="principal-component-analysis.html#cb569-22" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Loading (Correlation with PC)&quot;</span></span>
<span id="cb569-23"><a href="principal-component-analysis.html#cb569-23" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb569-24"><a href="principal-component-analysis.html#cb569-24" tabindex="-1"></a>  <span class="fu">theme_psych_book</span>() <span class="sc">+</span></span>
<span id="cb569-25"><a href="principal-component-analysis.html#cb569-25" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">60</span>, <span class="at">hjust =</span> <span class="dv">1</span>, <span class="at">size =</span> <span class="fu">rel</span>(<span class="fl">0.8</span>)), <span class="co"># Improve x-axis label readability</span></span>
<span id="cb569-26"><a href="principal-component-analysis.html#cb569-26" tabindex="-1"></a>        <span class="at">legend.position =</span> <span class="st">&quot;none&quot;</span>, <span class="co"># Fill is mapped to component, already clear from facets</span></span>
<span id="cb569-27"><a href="principal-component-analysis.html#cb569-27" tabindex="-1"></a>        <span class="at">strip.background =</span> <span class="fu">element_rect</span>(<span class="at">fill =</span> cool_colors[<span class="dv">6</span>], <span class="at">color =</span> <span class="st">&quot;grey70&quot;</span>), <span class="co"># Style facet titles</span></span>
<span id="cb569-28"><a href="principal-component-analysis.html#cb569-28" tabindex="-1"></a>        <span class="at">strip.text =</span> <span class="fu">element_text</span>(<span class="at">color =</span> <span class="st">&quot;white&quot;</span>, <span class="at">face =</span> <span class="st">&quot;bold&quot;</span>))</span>
<span id="cb569-29"><a href="principal-component-analysis.html#cb569-29" tabindex="-1"></a></span>
<span id="cb569-30"><a href="principal-component-analysis.html#cb569-30" tabindex="-1"></a><span class="fu">print</span>(loadings_plot_gg)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:pca-r-loadings"></span>
<img src="psychological_measurement_handbook_files/figure-html/pca-r-loadings-1.png" alt="Bar plot of item loadings on the first three principal components. Each panel shows loadings for one PC. Items are expected to group according to the simulated constructs (Social Engagement, Emotional Stability, Task Focus)." width="672" />
<p class="caption">
Figure 7.3: Bar plot of item loadings on the first three principal components. Each panel shows loadings for one PC. Items are expected to group according to the simulated constructs (Social Engagement, Emotional Stability, Task Focus).
</p>
</div>
<div class="sourceCode" id="cb570"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb570-1"><a href="principal-component-analysis.html#cb570-1" tabindex="-1"></a><span class="co"># Alternative: Heatmap of loadings for a compact view</span></span>
<span id="cb570-2"><a href="principal-component-analysis.html#cb570-2" tabindex="-1"></a>loadings_heatmap_gg <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(loadings_long_df, <span class="fu">aes</span>(<span class="at">x =</span> Item, <span class="at">y =</span> Component, <span class="at">fill =</span> Loading)) <span class="sc">+</span></span>
<span id="cb570-3"><a href="principal-component-analysis.html#cb570-3" tabindex="-1"></a>  <span class="fu">geom_tile</span>(<span class="at">color =</span> <span class="st">&quot;white&quot;</span>) <span class="sc">+</span> <span class="co"># Add white borders to tiles</span></span>
<span id="cb570-4"><a href="principal-component-analysis.html#cb570-4" tabindex="-1"></a>  <span class="fu">scale_fill_gradient2</span>(<span class="at">low =</span> cool_colors[<span class="dv">1</span>], <span class="at">mid =</span> <span class="st">&quot;white&quot;</span>, <span class="at">high =</span> cool_colors[<span class="dv">5</span>], <span class="at">midpoint =</span> <span class="dv">0</span>,</span>
<span id="cb570-5"><a href="principal-component-analysis.html#cb570-5" tabindex="-1"></a>                       <span class="at">name =</span> <span class="st">&quot;Loading&quot;</span>) <span class="sc">+</span></span>
<span id="cb570-6"><a href="principal-component-analysis.html#cb570-6" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="fu">aes</span>(<span class="at">label =</span> <span class="fu">round</span>(Loading, <span class="dv">2</span>)), <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">size =</span> <span class="fl">2.5</span>) <span class="sc">+</span> <span class="co"># Add loading values</span></span>
<span id="cb570-7"><a href="principal-component-analysis.html#cb570-7" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb570-8"><a href="principal-component-analysis.html#cb570-8" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Heatmap of Item Loadings on Principal Components&quot;</span>,</span>
<span id="cb570-9"><a href="principal-component-analysis.html#cb570-9" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Questionnaire Item&quot;</span>,</span>
<span id="cb570-10"><a href="principal-component-analysis.html#cb570-10" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Principal Component&quot;</span></span>
<span id="cb570-11"><a href="principal-component-analysis.html#cb570-11" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb570-12"><a href="principal-component-analysis.html#cb570-12" tabindex="-1"></a>  <span class="fu">theme_psych_book</span>() <span class="sc">+</span></span>
<span id="cb570-13"><a href="principal-component-analysis.html#cb570-13" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">60</span>, <span class="at">hjust =</span> <span class="dv">1</span>, <span class="at">size =</span> <span class="fu">rel</span>(<span class="fl">0.8</span>)),</span>
<span id="cb570-14"><a href="principal-component-analysis.html#cb570-14" tabindex="-1"></a>        <span class="at">panel.grid.major =</span> <span class="fu">element_blank</span>(), <span class="co"># Remove grid lines for heatmap</span></span>
<span id="cb570-15"><a href="principal-component-analysis.html#cb570-15" tabindex="-1"></a>        <span class="at">panel.border =</span> <span class="fu">element_rect</span>(<span class="at">colour =</span> <span class="st">&quot;grey70&quot;</span>, <span class="at">fill=</span><span class="cn">NA</span>, <span class="at">linewidth=</span><span class="dv">1</span>)) <span class="co"># Ensure border</span></span>
<span id="cb570-16"><a href="principal-component-analysis.html#cb570-16" tabindex="-1"></a></span>
<span id="cb570-17"><a href="principal-component-analysis.html#cb570-17" tabindex="-1"></a><span class="co"># print(loadings_heatmap_gg) # Uncomment to show heatmap</span></span></code></pre></div>
<p><strong>Explanation of Loadings:</strong>
The loadings matrix (and its visualization) shows the correlation between each original questionnaire item and each of the retained principal components. These values are key to interpreting what each component represents.
- <strong>Interpreting PC1</strong>: We examine which items have high positive or negative loadings on PC1 (typically, absolute values &gt; 0.4 or 0.5 are considered significant). The content of these high-loading items defines the meaning of PC1.
- <strong>Interpreting PC2 &amp; PC3</strong>: We repeat this process for PC2 and PC3, focusing on items that load highly on these components, preferably those that didn't load as strongly on previously interpreted components.</p>
<p><strong>Expected Pattern for Simulated Data:</strong>
Given our data simulation, we expect a “simple structure” where:
- Items 1-4 (designed for Social Engagement) should load highly on one of the three PCs.
- Items 5-8 (designed for Emotional Stability) should load highly on a <em>different</em> PC.
- Items 9-12 (designed for Task Focus) should load highly on the <em>remaining</em> PC.</p>
<p>The actual order (e.g., whether Social Engagement aligns with PC1, PC2, or PC3) depends on which construct explains the most variance. The bar plot (faceted by component) and the heatmap help visualize this pattern. By examining which group of items defines each PC, we can assign a meaningful psychological label to that component (e.g., if PC1 is strongly associated with Items 1-4, we might label it “Social Engagement Factor”). The goal is to identify components that are both statistically robust and theoretically meaningful.</p>
</div>
<div id="component-scores" class="section level2 hasAnchor" number="7.12">
<h2><span class="header-section-number">7.12</span> Component Scores<a href="principal-component-analysis.html#component-scores" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Once the principal components are identified and interpreted, we can calculate scores for each participant on these new dimensions. These scores represent each individual's standing on the derived, uncorrelated constructs. <code>prcomp</code> conveniently stores these scores in <code>pca_results$x</code>.</p>
<div class="sourceCode" id="cb571"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb571-1"><a href="principal-component-analysis.html#cb571-1" tabindex="-1"></a><span class="co"># Component scores are in pca_results$x</span></span>
<span id="cb571-2"><a href="principal-component-analysis.html#cb571-2" tabindex="-1"></a><span class="co"># We select the scores for the retained components</span></span>
<span id="cb571-3"><a href="principal-component-analysis.html#cb571-3" tabindex="-1"></a>component_scores_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(pca_results<span class="sc">$</span>x[, <span class="dv">1</span><span class="sc">:</span>num_components_to_retain])</span>
<span id="cb571-4"><a href="principal-component-analysis.html#cb571-4" tabindex="-1"></a></span>
<span id="cb571-5"><a href="principal-component-analysis.html#cb571-5" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">First 6 rows of Component Scores (for first&quot;</span>, num_components_to_retain, <span class="st">&quot;PCs):</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>
First 6 rows of Component Scores (for first 3 PCs):</code></pre>
<div class="sourceCode" id="cb573"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb573-1"><a href="principal-component-analysis.html#cb573-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">head</span>(component_scores_df))</span></code></pre></div>
<pre><code>          PC1         PC2        PC3
1 -2.36672255 -1.60959203  1.0964225
2 -0.24598991 -2.24673834  0.2569511
3  2.98529620 -1.31650752  1.5878553
4 -0.04368683 -0.08277365 -1.5429508
5  0.89975233  1.27548034 -0.3274003
6  3.42016260  1.58305726  0.7360223</code></pre>
<div class="sourceCode" id="cb575"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb575-1"><a href="principal-component-analysis.html#cb575-1" tabindex="-1"></a><span class="co"># These scores can be used in further analyses:</span></span>
<span id="cb575-2"><a href="principal-component-analysis.html#cb575-2" tabindex="-1"></a><span class="co"># - Correlating them with other psychological measures or demographic variables.</span></span>
<span id="cb575-3"><a href="principal-component-analysis.html#cb575-3" tabindex="-1"></a><span class="co"># - Using them as predictors or outcomes in regression models.</span></span>
<span id="cb575-4"><a href="principal-component-analysis.html#cb575-4" tabindex="-1"></a><span class="co"># - Input for clustering algorithms to identify participant subgroups.</span></span>
<span id="cb575-5"><a href="principal-component-analysis.html#cb575-5" tabindex="-1"></a></span>
<span id="cb575-6"><a href="principal-component-analysis.html#cb575-6" tabindex="-1"></a><span class="co"># Visualize the distribution of scores for PC1 and PC2</span></span>
<span id="cb575-7"><a href="principal-component-analysis.html#cb575-7" tabindex="-1"></a><span class="co"># (assuming num_components_to_retain &gt;= 2)</span></span>
<span id="cb575-8"><a href="principal-component-analysis.html#cb575-8" tabindex="-1"></a><span class="cf">if</span> (num_components_to_retain <span class="sc">&gt;=</span> <span class="dv">2</span>) {</span>
<span id="cb575-9"><a href="principal-component-analysis.html#cb575-9" tabindex="-1"></a>  scores_plot_gg <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(component_scores_df, <span class="fu">aes</span>(<span class="at">x =</span> PC1, <span class="at">y =</span> PC2)) <span class="sc">+</span></span>
<span id="cb575-10"><a href="principal-component-analysis.html#cb575-10" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.6</span>, <span class="at">color =</span> cool_colors[<span class="dv">2</span>], <span class="at">shape =</span> <span class="dv">16</span>) <span class="sc">+</span></span>
<span id="cb575-11"><a href="principal-component-analysis.html#cb575-11" tabindex="-1"></a>    <span class="fu">labs</span>(</span>
<span id="cb575-12"><a href="principal-component-analysis.html#cb575-12" tabindex="-1"></a>      <span class="at">title =</span> <span class="st">&quot;Participant Scores on PC1 and PC2&quot;</span>,</span>
<span id="cb575-13"><a href="principal-component-analysis.html#cb575-13" tabindex="-1"></a>      <span class="at">x =</span> <span class="fu">paste</span>(<span class="st">&quot;Scores on Principal Component 1 (explains &quot;</span>, <span class="fu">round</span>(prop_variance_explained[<span class="dv">1</span>]<span class="sc">*</span><span class="dv">100</span>,<span class="dv">1</span>), <span class="st">&quot;%)&quot;</span>, <span class="at">sep=</span><span class="st">&quot;&quot;</span>),</span>
<span id="cb575-14"><a href="principal-component-analysis.html#cb575-14" tabindex="-1"></a>      <span class="at">y =</span> <span class="fu">paste</span>(<span class="st">&quot;Scores on Principal Component 2 (explains &quot;</span>, <span class="fu">round</span>(prop_variance_explained[<span class="dv">2</span>]<span class="sc">*</span><span class="dv">100</span>,<span class="dv">1</span>), <span class="st">&quot;%)&quot;</span>, <span class="at">sep=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb575-15"><a href="principal-component-analysis.html#cb575-15" tabindex="-1"></a>    ) <span class="sc">+</span></span>
<span id="cb575-16"><a href="principal-component-analysis.html#cb575-16" tabindex="-1"></a>    <span class="fu">theme_psych_book</span>() <span class="sc">+</span></span>
<span id="cb575-17"><a href="principal-component-analysis.html#cb575-17" tabindex="-1"></a>    <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>, <span class="at">linetype=</span><span class="st">&quot;dashed&quot;</span>, <span class="at">color=</span><span class="st">&quot;grey70&quot;</span>) <span class="sc">+</span></span>
<span id="cb575-18"><a href="principal-component-analysis.html#cb575-18" tabindex="-1"></a>    <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>, <span class="at">linetype=</span><span class="st">&quot;dashed&quot;</span>, <span class="at">color=</span><span class="st">&quot;grey70&quot;</span>)</span>
<span id="cb575-19"><a href="principal-component-analysis.html#cb575-19" tabindex="-1"></a>  </span>
<span id="cb575-20"><a href="principal-component-analysis.html#cb575-20" tabindex="-1"></a>  <span class="fu">print</span>(scores_plot_gg)</span>
<span id="cb575-21"><a href="principal-component-analysis.html#cb575-21" tabindex="-1"></a>}</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:pca-r-scores"></span>
<img src="psychological_measurement_handbook_files/figure-html/pca-r-scores-1.png" alt="Scatter plot of participant scores on PC1 and PC2. This visualization can reveal clusters or patterns in how individuals score on the primary dimensions of variation." width="672" />
<p class="caption">
Figure 7.4: Scatter plot of participant scores on PC1 and PC2. This visualization can reveal clusters or patterns in how individuals score on the primary dimensions of variation.
</p>
</div>
<p><strong>Explanation of Component Scores:</strong>
The <code>component_scores_df</code> contains the new set of values for each participant on the retained principal components (PC1, PC2, PC3). Each column represents a component, and each row corresponds to a participant.
- These scores are standardized (mean 0, variance equal to the eigenvalue).
- They are uncorrelated with each other.
- They can be used as new variables in subsequent statistical analyses, effectively reducing the dimensionality of the original dataset from 12 items to 3 components while retaining most of the important information.
The scatter plot of PC1 vs. PC2 scores helps visualize the distribution of participants in this new two-dimensional space. If distinct clusters of participants emerge, it might suggest different psychological profiles based on these primary components.</p>
</div>
<div id="assumptions-and-limitations-of-pca" class="section level2 hasAnchor" number="7.13">
<h2><span class="header-section-number">7.13</span> Assumptions and Limitations of PCA<a href="principal-component-analysis.html#assumptions-and-limitations-of-pca" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>PCA is a powerful tool, but its effective application depends on understanding its underlying assumptions and limitations:</p>
<ol style="list-style-type: decimal">
<li><strong>Linearity</strong>: PCA assumes that the principal components are linear combinations of the original variables and that the relationships among variables are linear. If underlying relationships are strongly non-linear, PCA may not effectively capture the data structure. Non-linear dimensionality reduction techniques (e.g., t-SNE, UMAP) might be more appropriate in such cases.</li>
<li><strong>Scale of Measurement</strong>: Variables should ideally be measured on at least an interval scale for covariances and correlations to be meaningful. Applying PCA to ordinal data (e.g., Likert scales) is common in psychology but is a subject of debate. For ordinal data, Polychoric PCA (PCA on a polychoric correlation matrix) is often recommended as it better models the underlying continuous nature of ordinal responses.</li>
<li><strong>Sufficient Correlation</strong>: PCA is most useful when the original variables are at least moderately correlated. If variables are largely uncorrelated, PCA will not achieve significant dimensionality reduction, and components will mostly align with the original variables, each explaining little variance beyond that of a single variable.</li>
<li><strong>Adequate Sample Size</strong>: While there are no absolute rules, larger sample sizes are generally preferred for stable PCA results. Common rules of thumb include having at least 5-10 observations per variable (e.g., for 12 variables, 60-120 participants) or an overall sample size of N &gt; 100 or N &gt; 200. Stability of loadings is a key concern with small samples.</li>
<li><strong>Absence of Severe Outliers</strong>: PCA can be sensitive to outliers, as they can disproportionately influence the calculation of variances and covariances, and thus the orientation of principal components. It's good practice to screen for and handle outliers appropriately before PCA.</li>
<li><strong>Normality (for inference, not for description)</strong>: PCA itself, as a descriptive technique for variance decomposition, does not formally assume multivariate normality of the data. However, if inferential tests are applied to PCA results (e.g., significance tests for loadings, though less common in basic PCA), normality assumptions might become relevant. Gross deviations from normality can affect the stability of the solution.</li>
<li><strong>Sensitivity to Variable Scaling</strong>: As highlighted, PCA is sensitive to the scaling of variables if performed on the covariance matrix. Standardizing variables (i.e., performing PCA on the correlation matrix) is the standard approach when variables are measured on different scales or have vastly different variances not reflective of their importance.</li>
<li><strong>Interpretation Subjectivity</strong>: While loadings provide quantitative information, the interpretation and labeling of components often involve a degree of subjectivity and require substantive domain expertise. Different researchers might interpret the same component slightly differently.</li>
<li><strong>Information Loss</strong>: When reducing dimensionality (i.e., retaining <span class="math inline">\(k &lt; p\)</span> components), some information (variance) from the original dataset is inevitably lost. The goal is to ensure that the lost information is minimal and primarily noise, while the retained components capture the essential structure.</li>
<li><strong>Focus on Variance, Not Necessarily Latent Structure</strong>: PCA aims to maximize variance explained by components. It does not explicitly model latent constructs in the way Factor Analysis does. If the goal is to identify underlying unobserved factors causing the correlations, Factor Analysis might be more appropriate.</li>
</ol>
</div>
<div id="pca-vs.-factor-analysis-fa" class="section level2 hasAnchor" number="7.14">
<h2><span class="header-section-number">7.14</span> PCA vs. Factor Analysis (FA)<a href="principal-component-analysis.html#pca-vs.-factor-analysis-fa" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>PCA is often confused with Factor Analysis (FA), particularly Exploratory Factor Analysis (EFA), as both are dimensionality reduction techniques used to explore the structure of a set of variables. However, they have distinct conceptual foundations, mathematical models, and goals:</p>
<table>
<colgroup>
<col width="13%" />
<col width="42%" />
<col width="44%" />
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>Principal Component Analysis (PCA)</th>
<th>Exploratory Factor Analysis (EFA)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Primary Goal</strong></td>
<td>Data reduction and summarization. Maximize total variance explained by components.</td>
<td>Identify underlying latent factors that cause observed variable correlations. Explain common variance.</td>
</tr>
<tr class="even">
<td><strong>Mathematical Model</strong></td>
<td><span class="math inline">\(PC_j = w_{j1}X_1 + ... + w_{jp}X_p\)</span> (Components are linear combinations of observed variables). Assumes no measurement error in the components themselves.</td>
<td><span class="math inline">\(X_j = l_{j1}F_1 + ... + l_{jk}F_k + e_j\)</span> (Observed variables are linear combinations of latent factors plus unique error). Explicitly models measurement error.</td>
</tr>
<tr class="odd">
<td><strong>Variance Analyzed</strong></td>
<td>Decomposes <strong>total variance</strong> of the variables.</td>
<td>Decomposes <strong>common variance</strong> (shared among variables), separating it from unique variance (specific to each variable + error).</td>
</tr>
<tr class="even">
<td><strong>Input Matrix</strong></td>
<td>Typically uses a covariance or correlation matrix with 1s on the diagonal.</td>
<td>Uses a correlation matrix, often with communality estimates on the diagonal (instead of 1s) to focus on shared variance.</td>
</tr>
<tr class="odd">
<td><strong>Nature of Result</strong></td>
<td>Components are weighted composites of observed variables.</td>
<td>Factors are hypothetical, unobserved latent constructs.</td>
</tr>
<tr class="even">
<td><strong>Use Case</strong></td>
<td>When the primary aim is to reduce variables to a smaller, manageable set for prediction or description, without strong assumptions about underlying constructs.</td>
<td>When theory suggests latent constructs underlie observed variables, and the goal is to uncover or test this factor structure.</td>
</tr>
<tr class="odd">
<td><strong>Terminology</strong></td>
<td>“Components,” “Loadings” (often correlations between variables and components).</td>
<td>“Factors,” “Factor Loadings” (regression-like coefficients of variables on factors, or correlations).</td>
</tr>
</tbody>
</table>
<p><strong>Key Differences Summarized:</strong>
- <strong>PCA explains total variance; EFA explains common (shared) variance.</strong> PCA assumes all variance is “signal” to be captured. EFA distinguishes between variance shared with other variables (common) and variance unique to a variable (unique + error).
- <strong>PCA is a formative model; EFA is often a reflective model.</strong> In PCA, components are formed from variables. In EFA, variables are seen as reflections (indicators) of underlying factors.
- <strong>Communalities:</strong> EFA explicitly estimates communalities (the proportion of a variable's variance explained by the common factors). PCA, when based on a correlation matrix, implicitly assumes initial communalities are 1.</p>
<p>In practice, if variables are highly reliable and the number of variables is large, PCA and EFA can sometimes yield similar substantive interpretations of the dimensions. However, they are conceptually distinct, and the choice should be driven by the research question and theoretical assumptions about the data. If the goal is to understand latent constructs, EFA is generally preferred. If the goal is efficient data summarization, PCA is often suitable.</p>
</div>
<div id="conclusion-3" class="section level2 hasAnchor" number="7.15">
<h2><span class="header-section-number">7.15</span> Conclusion<a href="principal-component-analysis.html#conclusion-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Principal Component Analysis is a versatile and widely used statistical technique in psychology and other fields for simplifying complex, high-dimensional datasets. Its primary strength lies in its ability to reduce the number of variables while retaining most of the original information (variance), thereby uncovering the underlying structure in a more parsimonious way. By transforming a set of potentially correlated variables into a smaller set of uncorrelated principal components, PCA allows researchers to better understand and visualize patterns within their data.</p>
<p>Throughout this chapter, we have explored the mathematical foundations of PCA, delving into concepts such as the covariance/correlation matrix, eigenvectors, and eigenvalues, which are central to its operation. We demonstrated a practical application of PCA using R, covering data simulation, the execution of PCA, interpretation of key outputs like the scree plot and component loadings, and the selection of an appropriate number of components. Crucially, we also discussed the assumptions and limitations inherent in PCA, as well as its important distinctions from Factor Analysis, guiding appropriate application and interpretation.</p>
<p>When applied thoughtfully and with an understanding of its principles, PCA can provide invaluable insights. It aids in theory development by revealing hidden dimensions in psychological constructs, assists in the construction and refinement of measurement scales, and can improve the efficiency of predictive modeling by providing a reduced set of uncorrelated predictors. PCA remains a fundamental tool in the quantitative psychologist's toolkit, essential for navigating and making sense of the multifaceted nature of human behavior, cognition, and experience.</p>
</div>
<div id="references-5" class="section level2 hasAnchor" number="7.16">
<h2><span class="header-section-number">7.16</span> References<a href="principal-component-analysis.html#references-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>Jolliffe, I. T. (2002). <em>Principal Component Analysis</em> (2nd ed.). Springer. (Comprehensive statistical treatment)</li>
<li>Tabachnick, B. G., &amp; Fidell, L. S. (2019). <em>Using Multivariate Statistics</em> (7th ed.). Pearson. (Applied guide with examples, covers PCA and FA)</li>
<li>Field, A. P., Miles, J., &amp; Field, Z. (2012). <em>Discovering statistics using R</em>. Sage publications. (Accessible introduction with R examples)</li>
<li>Kaiser, H. F. (1960). The application of electronic computers to factor analysis. <em>Educational and Psychological Measurement, 20</em>(1), 141-151. (Introduced the Kaiser criterion)</li>
<li>Cattell, R. B. (1966). The scree test for the number of factors. <em>Multivariate Behavioral Research, 1</em>(2), 245-276. (Introduced the scree plot)</li>
<li>Horn, J. L. (1965). A rationale and test for the number of factors in factor analysis. <em>Psychometrika, 30</em>(2), 179-185. (Basis for Parallel Analysis)</li>
<li>Fabrigar, L. R., Wegener, D. T., MacCallum, R. C., &amp; Strahan, E. J. (1999). Evaluating the use of exploratory factor analysis in psychological research. <em>Psychological Methods, 4</em>(3), 272–299. (Excellent discussion on PCA vs. EFA and best practices)</li>
<li>O'Connor, B. P. (2000). SPSS and SAS programs for determining the number of components using parallel analysis and Velicer's MAP test. <em>Behavior Research Methods, Instruments, &amp; Computers, 32</em>(3), 396-402. (Practical guide for implementing Parallel Analysis, relevant for understanding its R implementations)</li>
<li>Costello, A. B., &amp; Osborne, J. (2005). Best practices in exploratory factor analysis: Four recommendations for getting the most from your analysis. <em>Practical Assessment, Research, and Evaluation, 10</em>(1), 7. (While focused on EFA, many recommendations are relevant for PCA data screening and interpretation)</li>
<li>Abdi, H., &amp; Williams, L. J. (2010). Principal component analysis. <em>Wiley Interdisciplinary Reviews: Computational Statistics, 2</em>(4), 433-459. (Good overview of PCA with mathematical details)</li>
<li>Jackson, J. E. (1991). <em>A User's Guide to Principal Components</em>. Wiley. (Classic, detailed text)</li>
<li>Zwick, W. R., &amp; Velicer, W. F. (1986). Comparison of five rules for determining the number of components to retain. <em>Psychological Bulletin, 99</em>(3), 432–442. (Empirical comparison of component retention rules)</li>
<li>Preacher, K. J., &amp; MacCallum, R. C. (2003). Repairing Tom Swift’s electric factor analysis machine. <em>Understanding Statistics, 2</em>(1), 13-43. (Critique and suggestions for better practice in factor analysis, relevant context for PCA users)</li>
<li>Bryant, F. B., &amp; Yarnold, P. R. (1995). Principal-components analysis and exploratory and confirmatory factor analysis. In L. G. Grimm &amp; P. R. Yarnold (Eds.), <em>Reading and understanding multivariate statistics</em> (pp. 99–136). American Psychological Association. (Chapter explaining PCA/FA for psychologists)</li>
<li>Ledesma, R. D., &amp; Valero-Mora, P. (2007). Determining the number of factors to retain in EFA: An easy-to-use computer program for carrying out Parallel Analysis. <em>Practical Assessment, Research &amp; Evaluation, 12</em>(2), 1-11. (Highlights utility of Parallel Analysis)</li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="validity-and-validation-tests.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chi-square.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": null,
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["psychological_measurement_handbook.pdf", "psychological_measurement_handbook.epub"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
